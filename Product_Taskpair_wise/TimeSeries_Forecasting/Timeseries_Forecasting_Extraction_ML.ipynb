{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189ff123",
   "metadata": {
    "id": "189ff123"
   },
   "source": [
    "# Welcome to the AIShield Integration Notebook!\n",
    "\n",
    "**We are delighted to have you here!**\n",
    "\n",
    "## What to Expect !!\n",
    "This notebook serves as a comprehensive demonstration of the seamless integration capabilities of AIShield. As a powerful tool for securing your AI/ML assets against adversarial threats, AIShield ensures the integrity and robustness of your models, preventing financial loss, protecting brand reputation, and mitigating the risk of intellectual property theft.\n",
    "\n",
    "## What to Do !!\n",
    "- **Explore the notebook** at your own pace to gain deep insights into the functionalities and features offered by AIShield.\n",
    "- **Run the first code cell/block**, which includes **Support functionalities** crucial for the smooth execution of the notebook. This setup will enhance your experience and enable the seamless integration of AIShield.\n",
    "- Even without running the entire notebook, you can get a glimpse of AIShield's capabilities and its approach to vulnerability assessments through sample scenarios.\n",
    "- **Run the code cells** to witness firsthand how AIShield seamlessly integrates with your existing codebase, empowering you to protect your AI models against adversarial attacks.\n",
    "- **Experiment and adapt the code** to suit your specific use cases, leveraging the flexibility and customization options provided by AIShield.\n",
    "- Should you have any questions or require assistance, our dedicated [AIShield.Contact@bosch.com](mailto:AIShield.Contact@bosch.com) is just a click away.\n",
    "## What You'll Get !!\n",
    "Throughout this notebook, you'll find:\n",
    "- Code snippets demonstrating step-by-step procedures for incorporating AIShield into your existing workflows, ensuring comprehensive protection against adversarial threats.\n",
    "- Detailed guidance on the integration process, including any additional inputs required for thorough vulnerability analysis.\n",
    "- A comprehensive threat-informed defense model, empowering you to understand the vulnerabilities in your AI models and undertake appropriate remediation measures.\n",
    "- Report artifacts and a sample attack dataset, providing valuable insights into potential attack scenarios and assisting in devising robust defense strategies.\n",
    "- In-depth explanations of defense remediation techniques, equipping you with the knowledge and tools to safeguard your AI assets effectively.\n",
    "\n",
    "## <span style=\"color:teal\">AIShield Website and LinkedIn</span>\n",
    "To learn more about AIShield and its cutting-edge features, visit the official [AIShield website](https://www.boschaishield.com/). Connect with AIShield on [LinkedIn](https://www.linkedin.com/company/bosch-aishield/about/) to stay updated on the latest advancements in AI security.\n",
    "\n",
    "We hope this notebook empowers you to seamlessly integrate AIShield into your projects, ensuring the utmost protection for your AI/ML assets. If you have any questions or need further assistance, please don't hesitate to reach out. **Happy exploring!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70f781",
   "metadata": {
    "id": "ef70f781"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bosch-aisecurity-aishield/Reference-Implementations/blob/main/Product_Taskpair_wise/TimeSeries_Forecasting/Timeseries_Forecasting_Extraction_ML.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05c7c5",
   "metadata": {
    "id": "bf05c7c5"
   },
   "source": [
    "## 1.0 Support Functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d309907f",
   "metadata": {
    "id": "d309907f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : Use PyPi or AIShield API for Vulnerability Analysis\n",
    "              if PyPi = True (this notebook will use aishield pypi package for vulnerability analysis)\n",
    "              else = False (this notebook will use aishield api for vulnerability analysis)\n",
    "\"\"\"\n",
    "pypi = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b4c1d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "a8b4c1d3",
    "outputId": "2a1a89f6-776d-4479-ce6e-991ac3981def"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "        .scrollable-code-input {\n",
       "            max-height: 500px;\n",
       "            overflow-y: scroll;\n",
       "        }\n",
       "        </style>\n",
       "        <script>\n",
       "        var codeInputCells = document.querySelectorAll('div.code_cell .input');\n",
       "        codeInputCells.forEach(function(cell) {\n",
       "            cell.classList.add('scrollable-code-input');\n",
       "        });\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_code_input_scrollable():\n",
    "    \"\"\"Description : To make the input code cell scrollable for jupyternotebook\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "        .scrollable-code-input {\n",
    "            max-height: 500px;\n",
    "            overflow-y: scroll;\n",
    "        }\n",
    "        </style>\n",
    "        <script>\n",
    "        var codeInputCells = document.querySelectorAll('div.code_cell .input');\n",
    "        codeInputCells.forEach(function(cell) {\n",
    "            cell.classList.add('scrollable-code-input');\n",
    "        });\n",
    "        </script>\n",
    "    '''))\n",
    "\n",
    "make_code_input_scrollable()\n",
    "\n",
    "def display_pdf(runtime_environment,output_filename, pdf_path=None, file_id=None):\n",
    "\n",
    "    \"\"\"Descrption : To embeded pdf file in the ntoebook\"\"\"\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    import os\n",
    "    output_file = output_filename+'.html'\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            saved_output = f.read()\n",
    "            display(HTML(saved_output))\n",
    "    else:\n",
    "        if 'colab' in runtime_environment.lower():\n",
    "            # Generate the Google Drive Viewer URL\n",
    "            if file_id is None:\n",
    "                print(pdf_path)\n",
    "                # Generate the Google Drive Viewer URL\n",
    "                drive_viewer_url = f'https://drive.google.com/viewerng/viewer?embedded=true&url={pdf_path}'\n",
    "                html_code = f'<iframe src=\"{drive_viewer_url}\" width=\"100%\" height=\"600px\"></iframe>'\n",
    "            else:\n",
    "                viewer_url = f\"https://drive.google.com/file/d/{file_id}/preview\"\n",
    "                html_code = f'<iframe src=\"{viewer_url}\" width=\"100%\" height=\"600px\"></iframe>'\n",
    "        else:\n",
    "            html_code = f'<embed src=\"{pdf_path}\" width=\"100%\" height=\"600px\" type=\"application/pdf\">'\n",
    "        # Save the output to a file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(html_code)\n",
    "\n",
    "        # Display the output\n",
    "        display(HTML(html_code))\n",
    "\n",
    "\n",
    "def check_runtime_environment():\n",
    "    \"\"\"\n",
    "    Description : Check notebook is running on colab or jupyter-notebook\n",
    "    \"\"\"\n",
    "    import os\n",
    "    colab = False\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        colab = True\n",
    "        return 'Colab' , colab\n",
    "    else:\n",
    "        return 'Jupyter Notebook' , colab\n",
    "\n",
    "# Check the runtime environment\n",
    "runtime_environment,colab_running  = check_runtime_environment()\n",
    "\n",
    "# print(\"Notebook is running in\", runtime_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0cbcc",
   "metadata": {
    "id": "63f0cbcc"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecbce3",
   "metadata": {
    "id": "23ecbce3"
   },
   "source": [
    "\n",
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/wk1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a18c1",
   "metadata": {
    "id": "d21a18c1"
   },
   "source": [
    "## 2.0 Below code cell/block performs Step 1 to 4 with following these sequential steps :\n",
    "**STEP 1 : Installing libraries:** The required libraries will be installed to provide necessary functionalities for the task at hand.\n",
    "\n",
    "**STEP 2 : Loading the dataset:** The dataset containing the relevant information will be loaded into the program, allowing access to the data for analysis and modeling. **Before Running the notebook need to download the dataset from the mentioned link : https://www.kaggle.com/code/robikscube/tutorial-time-series-forecasting-with-xgboost/input?select=PJME_hourly.csv**. **Once downloaded just place the zipped file to the notebook location.**\n",
    "\n",
    "**STEP 3 : Model development and training:** A sophisticated model will be developed to analyze and interpret the dataset. The model will then be trained using appropriate techniques to optimize its performance.\n",
    "\n",
    "**STEP 4 : Preparing the Data, Model, and Label:** Prior to further analysis, essential steps such as data preprocessing, model configuration, and label preparation will be carried out. This will ensure that the data, model, and labels are properly structured and ready for subsequent stages needed for aishield.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8cb535",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f8cb535",
    "outputId": "09a84350-20ae-469e-e7c4-2e7e5a0fb891",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started of installing all the prerequisites packages\n",
      "Requirement already satisfied: xgboost==1.6.2 in /usr/local/lib/python3.10/dist-packages (1.6.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.6.2) (1.22.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.6.2) (1.10.1)\n",
      "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.10/dist-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.1.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.10/dist-packages (from pandas==1.1.5) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from pandas==1.1.5) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.22.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.10.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.1.0)\n",
      "Requirement already satisfied: numpy==1.22 in /usr/local/lib/python3.10/dist-packages (1.22.0)\n",
      "Requirement already satisfied: pyminizip in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
      "Requirement already satisfied: requests==2.28.0 in /usr/local/lib/python3.10/dist-packages (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (2023.5.7)\n",
      "Requirement already satisfied: humanfriendly==9.2 in /usr/local/lib/python3.10/dist-packages (9.2)\n",
      "Requirement already satisfied: tqdm==4.61.1 in /usr/local/lib/python3.10/dist-packages (4.61.1)\n",
      "Requirement already satisfied: requests==2.28.0 in /usr/local/lib/python3.10/dist-packages (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (2023.5.7)\n",
      "Starting installing aishield library\n",
      "Requirement already satisfied: aishield in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from aishield) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed all the prerequisites packages\n",
      "Extraction completed.\n",
      "Dataset Downloaded Successfully\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Development and Training:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: /content/data\n",
      "Removed: /content/model\n",
      "Removed: /content/minmax\n",
      "Removed: /content/encrypt_model\n",
      "Removed: /content/reports\n",
      "Removed: /content/zip\n",
      "Failed to removed: /content/reports/sample_data\n",
      "directory /content/data created successfully\n",
      "directory /content/model created successfully\n",
      "directory /content/minmax created successfully\n",
      "directory /content/reports created successfully\n",
      "directory /content/encrypt_model created successfully\n",
      "directory /content/zip created successfully\n",
      "directory /content/reports/sample_data created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.13103\tvalidation_1-rmse:0.14142\n",
      "[1]\tvalidation_0-rmse:0.09309\tvalidation_1-rmse:0.10079\n",
      "[2]\tvalidation_0-rmse:0.06670\tvalidation_1-rmse:0.07240\n",
      "[3]\tvalidation_0-rmse:0.04852\tvalidation_1-rmse:0.05286\n",
      "[4]\tvalidation_0-rmse:0.03615\tvalidation_1-rmse:0.03957\n",
      "[5]\tvalidation_0-rmse:0.02790\tvalidation_1-rmse:0.03074\n",
      "[6]\tvalidation_0-rmse:0.02260\tvalidation_1-rmse:0.02509\n",
      "[7]\tvalidation_0-rmse:0.01926\tvalidation_1-rmse:0.02151\n",
      "[8]\tvalidation_0-rmse:0.01724\tvalidation_1-rmse:0.01940\n",
      "[9]\tvalidation_0-rmse:0.01600\tvalidation_1-rmse:0.01807\n",
      "[10]\tvalidation_0-rmse:0.01526\tvalidation_1-rmse:0.01731\n",
      "[11]\tvalidation_0-rmse:0.01476\tvalidation_1-rmse:0.01680\n",
      "[12]\tvalidation_0-rmse:0.01438\tvalidation_1-rmse:0.01643\n",
      "[13]\tvalidation_0-rmse:0.01409\tvalidation_1-rmse:0.01623\n",
      "[14]\tvalidation_0-rmse:0.01386\tvalidation_1-rmse:0.01608\n",
      "[15]\tvalidation_0-rmse:0.01371\tvalidation_1-rmse:0.01595\n",
      "[16]\tvalidation_0-rmse:0.01353\tvalidation_1-rmse:0.01583\n",
      "[17]\tvalidation_0-rmse:0.01332\tvalidation_1-rmse:0.01569\n",
      "[18]\tvalidation_0-rmse:0.01320\tvalidation_1-rmse:0.01564\n",
      "[19]\tvalidation_0-rmse:0.01307\tvalidation_1-rmse:0.01557\n",
      "[20]\tvalidation_0-rmse:0.01301\tvalidation_1-rmse:0.01552\n",
      "[21]\tvalidation_0-rmse:0.01288\tvalidation_1-rmse:0.01542\n",
      "[22]\tvalidation_0-rmse:0.01283\tvalidation_1-rmse:0.01540\n",
      "[23]\tvalidation_0-rmse:0.01276\tvalidation_1-rmse:0.01536\n",
      "[24]\tvalidation_0-rmse:0.01269\tvalidation_1-rmse:0.01533\n",
      "[25]\tvalidation_0-rmse:0.01259\tvalidation_1-rmse:0.01529\n",
      "[26]\tvalidation_0-rmse:0.01251\tvalidation_1-rmse:0.01526\n",
      "[27]\tvalidation_0-rmse:0.01243\tvalidation_1-rmse:0.01522\n",
      "[28]\tvalidation_0-rmse:0.01237\tvalidation_1-rmse:0.01518\n",
      "[29]\tvalidation_0-rmse:0.01234\tvalidation_1-rmse:0.01515\n",
      "[30]\tvalidation_0-rmse:0.01226\tvalidation_1-rmse:0.01512\n",
      "[31]\tvalidation_0-rmse:0.01219\tvalidation_1-rmse:0.01508\n",
      "[32]\tvalidation_0-rmse:0.01215\tvalidation_1-rmse:0.01507\n",
      "[33]\tvalidation_0-rmse:0.01205\tvalidation_1-rmse:0.01505\n",
      "[34]\tvalidation_0-rmse:0.01199\tvalidation_1-rmse:0.01502\n",
      "[35]\tvalidation_0-rmse:0.01193\tvalidation_1-rmse:0.01499\n",
      "[36]\tvalidation_0-rmse:0.01188\tvalidation_1-rmse:0.01496\n",
      "[37]\tvalidation_0-rmse:0.01180\tvalidation_1-rmse:0.01493\n",
      "[38]\tvalidation_0-rmse:0.01177\tvalidation_1-rmse:0.01491\n",
      "[39]\tvalidation_0-rmse:0.01167\tvalidation_1-rmse:0.01487\n",
      "[40]\tvalidation_0-rmse:0.01163\tvalidation_1-rmse:0.01484\n",
      "[41]\tvalidation_0-rmse:0.01157\tvalidation_1-rmse:0.01484\n",
      "[42]\tvalidation_0-rmse:0.01148\tvalidation_1-rmse:0.01478\n",
      "[43]\tvalidation_0-rmse:0.01142\tvalidation_1-rmse:0.01476\n",
      "[44]\tvalidation_0-rmse:0.01136\tvalidation_1-rmse:0.01475\n",
      "[45]\tvalidation_0-rmse:0.01133\tvalidation_1-rmse:0.01473\n",
      "[46]\tvalidation_0-rmse:0.01129\tvalidation_1-rmse:0.01472\n",
      "[47]\tvalidation_0-rmse:0.01125\tvalidation_1-rmse:0.01470\n",
      "[48]\tvalidation_0-rmse:0.01118\tvalidation_1-rmse:0.01468\n",
      "[49]\tvalidation_0-rmse:0.01114\tvalidation_1-rmse:0.01465\n",
      "[50]\tvalidation_0-rmse:0.01110\tvalidation_1-rmse:0.01464\n",
      "[51]\tvalidation_0-rmse:0.01105\tvalidation_1-rmse:0.01462\n",
      "[52]\tvalidation_0-rmse:0.01100\tvalidation_1-rmse:0.01460\n",
      "[53]\tvalidation_0-rmse:0.01097\tvalidation_1-rmse:0.01458\n",
      "[54]\tvalidation_0-rmse:0.01091\tvalidation_1-rmse:0.01458\n",
      "[55]\tvalidation_0-rmse:0.01089\tvalidation_1-rmse:0.01457\n",
      "[56]\tvalidation_0-rmse:0.01087\tvalidation_1-rmse:0.01455\n",
      "[57]\tvalidation_0-rmse:0.01084\tvalidation_1-rmse:0.01454\n",
      "[58]\tvalidation_0-rmse:0.01077\tvalidation_1-rmse:0.01450\n",
      "[59]\tvalidation_0-rmse:0.01075\tvalidation_1-rmse:0.01449\n",
      "[60]\tvalidation_0-rmse:0.01073\tvalidation_1-rmse:0.01447\n",
      "[61]\tvalidation_0-rmse:0.01071\tvalidation_1-rmse:0.01447\n",
      "[62]\tvalidation_0-rmse:0.01065\tvalidation_1-rmse:0.01448\n",
      "[63]\tvalidation_0-rmse:0.01060\tvalidation_1-rmse:0.01443\n",
      "[64]\tvalidation_0-rmse:0.01056\tvalidation_1-rmse:0.01442\n",
      "[65]\tvalidation_0-rmse:0.01052\tvalidation_1-rmse:0.01440\n",
      "[66]\tvalidation_0-rmse:0.01049\tvalidation_1-rmse:0.01439\n",
      "[67]\tvalidation_0-rmse:0.01044\tvalidation_1-rmse:0.01437\n",
      "[68]\tvalidation_0-rmse:0.01041\tvalidation_1-rmse:0.01435\n",
      "[69]\tvalidation_0-rmse:0.01037\tvalidation_1-rmse:0.01434\n",
      "[70]\tvalidation_0-rmse:0.01033\tvalidation_1-rmse:0.01432\n",
      "[71]\tvalidation_0-rmse:0.01032\tvalidation_1-rmse:0.01432\n",
      "[72]\tvalidation_0-rmse:0.01027\tvalidation_1-rmse:0.01431\n",
      "[73]\tvalidation_0-rmse:0.01024\tvalidation_1-rmse:0.01431\n",
      "[74]\tvalidation_0-rmse:0.01020\tvalidation_1-rmse:0.01429\n",
      "[75]\tvalidation_0-rmse:0.01018\tvalidation_1-rmse:0.01429\n",
      "[76]\tvalidation_0-rmse:0.01017\tvalidation_1-rmse:0.01428\n",
      "[77]\tvalidation_0-rmse:0.01012\tvalidation_1-rmse:0.01427\n",
      "[78]\tvalidation_0-rmse:0.01008\tvalidation_1-rmse:0.01425\n",
      "[79]\tvalidation_0-rmse:0.01001\tvalidation_1-rmse:0.01423\n",
      "[80]\tvalidation_0-rmse:0.00998\tvalidation_1-rmse:0.01422\n",
      "[81]\tvalidation_0-rmse:0.00993\tvalidation_1-rmse:0.01421\n",
      "[82]\tvalidation_0-rmse:0.00990\tvalidation_1-rmse:0.01419\n",
      "[83]\tvalidation_0-rmse:0.00987\tvalidation_1-rmse:0.01418\n",
      "[84]\tvalidation_0-rmse:0.00983\tvalidation_1-rmse:0.01415\n",
      "[85]\tvalidation_0-rmse:0.00979\tvalidation_1-rmse:0.01413\n",
      "[86]\tvalidation_0-rmse:0.00974\tvalidation_1-rmse:0.01410\n",
      "[87]\tvalidation_0-rmse:0.00971\tvalidation_1-rmse:0.01409\n",
      "[88]\tvalidation_0-rmse:0.00970\tvalidation_1-rmse:0.01408\n",
      "[89]\tvalidation_0-rmse:0.00966\tvalidation_1-rmse:0.01407\n",
      "[90]\tvalidation_0-rmse:0.00964\tvalidation_1-rmse:0.01406\n",
      "[91]\tvalidation_0-rmse:0.00962\tvalidation_1-rmse:0.01405\n",
      "[92]\tvalidation_0-rmse:0.00962\tvalidation_1-rmse:0.01405\n",
      "[93]\tvalidation_0-rmse:0.00956\tvalidation_1-rmse:0.01402\n",
      "[94]\tvalidation_0-rmse:0.00953\tvalidation_1-rmse:0.01401\n",
      "[95]\tvalidation_0-rmse:0.00950\tvalidation_1-rmse:0.01401\n",
      "[96]\tvalidation_0-rmse:0.00946\tvalidation_1-rmse:0.01400\n",
      "[97]\tvalidation_0-rmse:0.00942\tvalidation_1-rmse:0.01398\n",
      "[98]\tvalidation_0-rmse:0.00939\tvalidation_1-rmse:0.01397\n",
      "[99]\tvalidation_0-rmse:0.00936\tvalidation_1-rmse:0.01395\n",
      "[100]\tvalidation_0-rmse:0.00935\tvalidation_1-rmse:0.01394\n",
      "[101]\tvalidation_0-rmse:0.00932\tvalidation_1-rmse:0.01394\n",
      "[102]\tvalidation_0-rmse:0.00929\tvalidation_1-rmse:0.01394\n",
      "[103]\tvalidation_0-rmse:0.00928\tvalidation_1-rmse:0.01394\n",
      "[104]\tvalidation_0-rmse:0.00924\tvalidation_1-rmse:0.01392\n",
      "[105]\tvalidation_0-rmse:0.00923\tvalidation_1-rmse:0.01391\n",
      "[106]\tvalidation_0-rmse:0.00920\tvalidation_1-rmse:0.01392\n",
      "[107]\tvalidation_0-rmse:0.00919\tvalidation_1-rmse:0.01391\n",
      "[108]\tvalidation_0-rmse:0.00917\tvalidation_1-rmse:0.01390\n",
      "[109]\tvalidation_0-rmse:0.00915\tvalidation_1-rmse:0.01390\n",
      "[110]\tvalidation_0-rmse:0.00914\tvalidation_1-rmse:0.01390\n",
      "[111]\tvalidation_0-rmse:0.00909\tvalidation_1-rmse:0.01387\n",
      "[112]\tvalidation_0-rmse:0.00906\tvalidation_1-rmse:0.01388\n",
      "[113]\tvalidation_0-rmse:0.00903\tvalidation_1-rmse:0.01387\n",
      "[114]\tvalidation_0-rmse:0.00902\tvalidation_1-rmse:0.01387\n",
      "[115]\tvalidation_0-rmse:0.00900\tvalidation_1-rmse:0.01387\n",
      "[116]\tvalidation_0-rmse:0.00897\tvalidation_1-rmse:0.01387\n",
      "[117]\tvalidation_0-rmse:0.00897\tvalidation_1-rmse:0.01387\n",
      "[118]\tvalidation_0-rmse:0.00895\tvalidation_1-rmse:0.01386\n",
      "[119]\tvalidation_0-rmse:0.00893\tvalidation_1-rmse:0.01386\n",
      "[120]\tvalidation_0-rmse:0.00890\tvalidation_1-rmse:0.01385\n",
      "[121]\tvalidation_0-rmse:0.00889\tvalidation_1-rmse:0.01385\n",
      "[122]\tvalidation_0-rmse:0.00887\tvalidation_1-rmse:0.01384\n",
      "[123]\tvalidation_0-rmse:0.00884\tvalidation_1-rmse:0.01383\n",
      "[124]\tvalidation_0-rmse:0.00882\tvalidation_1-rmse:0.01383\n",
      "[125]\tvalidation_0-rmse:0.00879\tvalidation_1-rmse:0.01383\n",
      "[126]\tvalidation_0-rmse:0.00877\tvalidation_1-rmse:0.01381\n",
      "[127]\tvalidation_0-rmse:0.00876\tvalidation_1-rmse:0.01381\n",
      "[128]\tvalidation_0-rmse:0.00874\tvalidation_1-rmse:0.01380\n",
      "[129]\tvalidation_0-rmse:0.00871\tvalidation_1-rmse:0.01379\n",
      "[130]\tvalidation_0-rmse:0.00868\tvalidation_1-rmse:0.01377\n",
      "[131]\tvalidation_0-rmse:0.00867\tvalidation_1-rmse:0.01377\n",
      "[132]\tvalidation_0-rmse:0.00865\tvalidation_1-rmse:0.01376\n",
      "[133]\tvalidation_0-rmse:0.00864\tvalidation_1-rmse:0.01376\n",
      "[134]\tvalidation_0-rmse:0.00863\tvalidation_1-rmse:0.01376\n",
      "[135]\tvalidation_0-rmse:0.00861\tvalidation_1-rmse:0.01375\n",
      "[136]\tvalidation_0-rmse:0.00860\tvalidation_1-rmse:0.01375\n",
      "[137]\tvalidation_0-rmse:0.00859\tvalidation_1-rmse:0.01375\n",
      "[138]\tvalidation_0-rmse:0.00855\tvalidation_1-rmse:0.01373\n",
      "[139]\tvalidation_0-rmse:0.00851\tvalidation_1-rmse:0.01373\n",
      "[140]\tvalidation_0-rmse:0.00849\tvalidation_1-rmse:0.01373\n",
      "[141]\tvalidation_0-rmse:0.00847\tvalidation_1-rmse:0.01373\n",
      "[142]\tvalidation_0-rmse:0.00846\tvalidation_1-rmse:0.01372\n",
      "[143]\tvalidation_0-rmse:0.00845\tvalidation_1-rmse:0.01372\n",
      "[144]\tvalidation_0-rmse:0.00843\tvalidation_1-rmse:0.01372\n",
      "[145]\tvalidation_0-rmse:0.00842\tvalidation_1-rmse:0.01371\n",
      "[146]\tvalidation_0-rmse:0.00839\tvalidation_1-rmse:0.01371\n",
      "[147]\tvalidation_0-rmse:0.00837\tvalidation_1-rmse:0.01370\n",
      "[148]\tvalidation_0-rmse:0.00834\tvalidation_1-rmse:0.01369\n",
      "[149]\tvalidation_0-rmse:0.00832\tvalidation_1-rmse:0.01369\n",
      "[150]\tvalidation_0-rmse:0.00830\tvalidation_1-rmse:0.01369\n",
      "[151]\tvalidation_0-rmse:0.00829\tvalidation_1-rmse:0.01369\n",
      "[152]\tvalidation_0-rmse:0.00827\tvalidation_1-rmse:0.01368\n",
      "[153]\tvalidation_0-rmse:0.00824\tvalidation_1-rmse:0.01368\n",
      "[154]\tvalidation_0-rmse:0.00823\tvalidation_1-rmse:0.01368\n",
      "[155]\tvalidation_0-rmse:0.00821\tvalidation_1-rmse:0.01368\n",
      "[156]\tvalidation_0-rmse:0.00820\tvalidation_1-rmse:0.01366\n",
      "[157]\tvalidation_0-rmse:0.00817\tvalidation_1-rmse:0.01367\n",
      "[158]\tvalidation_0-rmse:0.00816\tvalidation_1-rmse:0.01366\n",
      "[159]\tvalidation_0-rmse:0.00812\tvalidation_1-rmse:0.01366\n",
      "[160]\tvalidation_0-rmse:0.00809\tvalidation_1-rmse:0.01365\n",
      "[161]\tvalidation_0-rmse:0.00808\tvalidation_1-rmse:0.01365\n",
      "[162]\tvalidation_0-rmse:0.00806\tvalidation_1-rmse:0.01364\n",
      "[163]\tvalidation_0-rmse:0.00806\tvalidation_1-rmse:0.01364\n",
      "[164]\tvalidation_0-rmse:0.00804\tvalidation_1-rmse:0.01363\n",
      "[165]\tvalidation_0-rmse:0.00803\tvalidation_1-rmse:0.01363\n",
      "[166]\tvalidation_0-rmse:0.00802\tvalidation_1-rmse:0.01362\n",
      "[167]\tvalidation_0-rmse:0.00801\tvalidation_1-rmse:0.01362\n",
      "[168]\tvalidation_0-rmse:0.00799\tvalidation_1-rmse:0.01362\n",
      "[169]\tvalidation_0-rmse:0.00797\tvalidation_1-rmse:0.01361\n",
      "[170]\tvalidation_0-rmse:0.00795\tvalidation_1-rmse:0.01360\n",
      "[171]\tvalidation_0-rmse:0.00793\tvalidation_1-rmse:0.01360\n",
      "[172]\tvalidation_0-rmse:0.00791\tvalidation_1-rmse:0.01361\n",
      "[173]\tvalidation_0-rmse:0.00789\tvalidation_1-rmse:0.01360\n",
      "[174]\tvalidation_0-rmse:0.00788\tvalidation_1-rmse:0.01360\n",
      "[175]\tvalidation_0-rmse:0.00788\tvalidation_1-rmse:0.01360\n",
      "[176]\tvalidation_0-rmse:0.00786\tvalidation_1-rmse:0.01359\n",
      "[177]\tvalidation_0-rmse:0.00784\tvalidation_1-rmse:0.01359\n",
      "[178]\tvalidation_0-rmse:0.00782\tvalidation_1-rmse:0.01359\n",
      "[179]\tvalidation_0-rmse:0.00781\tvalidation_1-rmse:0.01358\n",
      "[180]\tvalidation_0-rmse:0.00779\tvalidation_1-rmse:0.01358\n",
      "[181]\tvalidation_0-rmse:0.00778\tvalidation_1-rmse:0.01358\n",
      "[182]\tvalidation_0-rmse:0.00775\tvalidation_1-rmse:0.01357\n",
      "[183]\tvalidation_0-rmse:0.00773\tvalidation_1-rmse:0.01357\n",
      "[184]\tvalidation_0-rmse:0.00773\tvalidation_1-rmse:0.01356\n",
      "[185]\tvalidation_0-rmse:0.00770\tvalidation_1-rmse:0.01356\n",
      "[186]\tvalidation_0-rmse:0.00769\tvalidation_1-rmse:0.01356\n",
      "[187]\tvalidation_0-rmse:0.00767\tvalidation_1-rmse:0.01356\n",
      "[188]\tvalidation_0-rmse:0.00766\tvalidation_1-rmse:0.01355\n",
      "[189]\tvalidation_0-rmse:0.00765\tvalidation_1-rmse:0.01355\n",
      "[190]\tvalidation_0-rmse:0.00763\tvalidation_1-rmse:0.01354\n",
      "[191]\tvalidation_0-rmse:0.00763\tvalidation_1-rmse:0.01354\n",
      "[192]\tvalidation_0-rmse:0.00762\tvalidation_1-rmse:0.01354\n",
      "[193]\tvalidation_0-rmse:0.00760\tvalidation_1-rmse:0.01353\n",
      "[194]\tvalidation_0-rmse:0.00758\tvalidation_1-rmse:0.01353\n",
      "[195]\tvalidation_0-rmse:0.00757\tvalidation_1-rmse:0.01353\n",
      "[196]\tvalidation_0-rmse:0.00755\tvalidation_1-rmse:0.01352\n",
      "[197]\tvalidation_0-rmse:0.00753\tvalidation_1-rmse:0.01351\n",
      "[198]\tvalidation_0-rmse:0.00752\tvalidation_1-rmse:0.01351\n",
      "[199]\tvalidation_0-rmse:0.00752\tvalidation_1-rmse:0.01351\n",
      "[200]\tvalidation_0-rmse:0.00750\tvalidation_1-rmse:0.01351\n",
      "[201]\tvalidation_0-rmse:0.00749\tvalidation_1-rmse:0.01351\n",
      "[202]\tvalidation_0-rmse:0.00748\tvalidation_1-rmse:0.01351\n",
      "[203]\tvalidation_0-rmse:0.00747\tvalidation_1-rmse:0.01351\n",
      "RMSE score of the Decision Tree model is 0.01350891481750004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AIShield API Call:  75%|███████▌  | 3/4 [06:20<02:06, 126.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below Cells will see How to integrate and Call AIShield for Vulnerability Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Show/Hide Code\n",
    "'''\n",
    "Description: user input about dataset information, for eg: number of classes, input_shape\n",
    "'''\n",
    "input_shape = (100,1)\n",
    "model_encryption=0 #0 if model is uploaded directly as a zip, 1 if model is encryted as .pyc and uploaded as a zip\n",
    "run_code = True\n",
    "install = True #to install the prerequisite libraries or not\n",
    "\n",
    "\n",
    "## installing libraries if not and then importing\n",
    "if install:\n",
    "    print(\"Started of installing all the prerequisites packages\")\n",
    "    !pip install xgboost==1.6.2\n",
    "    !pip install pandas==1.1.5\n",
    "    !pip install scikit-learn==1.0.2\n",
    "    !pip install numpy==1.22\n",
    "    !pip install pyminizip\n",
    "    !pip install requests==2.28.0\n",
    "    !pip install humanfriendly==9.2\n",
    "    !pip install tqdm==4.61.1\n",
    "    !pip install requests==2.28.0\n",
    "\n",
    "    print(\"Starting installing aishield library\")\n",
    "    !pip install aishield\n",
    "    print(\"Installed all the prerequisites packages\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipped the installation of all the prerequisites packages\")\n",
    "\n",
    "\n",
    "import io\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import shutil\n",
    "import requests\n",
    "import requests\n",
    "import py_compile\n",
    "# import pyminizip\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import aishield as ais\n",
    "\n",
    "\n",
    "\n",
    "#extract content from the nested zip file\n",
    "def extract_nested_zip(file_path, file_name=\"PJME_hourly.csv\"):\n",
    "    # Look for nested zip files\n",
    "    zip_file = zipfile.ZipFile(file_path)\n",
    "    current_path = os.getcwd()\n",
    "    # loading the temp.zip and creating a zip object\n",
    "    with ZipFile(file_path, 'r') as zObject:\n",
    "\n",
    "    # Extracting specific file in the zip\n",
    "    # into a specific location.\n",
    "        zObject.extract(file_name, path=current_path)\n",
    "    zObject.close()\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "\n",
    "def series_to_supervised(data, n_vars_out, n_in, n_out=1, fillnan=True):\n",
    "    '''\n",
    "    convert series to supervised learning\n",
    "    '''\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i).iloc[:,n_vars_out])\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in (n_vars_out)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in (n_vars_out)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # Replace nan values with 0\n",
    "    if fillnan:\n",
    "        agg = agg.fillna(0)\n",
    "    return agg\n",
    "\n",
    "def data_prep(df, input_dims, pred_out):\n",
    "    # convert input dims to number of timesteps and number of features\n",
    "    num_timesteps, num_features = input_shape[0], input_shape[1]#input_dims\n",
    "\n",
    "    # generate data samples by window sliding through time series data\n",
    "    reframed = series_to_supervised(df, pred_out, num_timesteps)\n",
    "    data = reframed.values\n",
    "\n",
    "    # split into input and outputs\n",
    "    n_obs = num_timesteps*num_features  # no. of timesteps * features\n",
    "    data_x, data_y = data[:, :n_obs], data[:, n_obs:]\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    data_x = data_x.reshape((data_x.shape[0], num_timesteps, num_features))\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "def load_data(zipped_file_path, file_name):\n",
    "\n",
    "        '''\n",
    "        Description :\n",
    "            Load the dataset.\n",
    "\n",
    "        input_parameter: None\n",
    "        return_parameter: return the training and testing the dataset\n",
    "            X_train, y_train , X_test, y_test\n",
    "\n",
    "        '''\n",
    "        '''\n",
    "        Description : Loading Hourly Energy Consumption dataset\n",
    "        '''\n",
    "        try:\n",
    "            extract_nested_zip(zipped_file_path)\n",
    "            print(\"Dataset Downloaded Successfully\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_name, index_col=[0], parse_dates=[0])\n",
    "\n",
    "        print(\"Dataset Loaded\")\n",
    "\n",
    "        return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    #scaling the dataset\n",
    "    values = df.values\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "\n",
    "    '''\n",
    "    Description : convert normalized dataset to a dataframe\n",
    "    '''\n",
    "\n",
    "    df_scaled = pd.DataFrame(scaled,columns = df.columns )\n",
    "\n",
    "    '''\n",
    "    Description : choosing train and testing dataset\n",
    "    '''\n",
    "\n",
    "    df_train = df_scaled.iloc[0:113927].copy()\n",
    "    df_test = df_scaled.iloc[113927:145366].copy()\n",
    "\n",
    "    '''\n",
    "    Description : Splitting data for training and testing\n",
    "    '''\n",
    "    pred_col_index = [len(df_train.columns) - 1]\n",
    "    X_train, y_train = data_prep(df_train, input_shape, pred_col_index)\n",
    "    X_test, y_test = data_prep(df_test, input_shape, pred_col_index)\n",
    "\n",
    "\n",
    "    return  X_train, X_test, y_train, y_test, df_test\n",
    "\n",
    "def make_directory(directory):\n",
    "    \"\"\"\n",
    "    Create directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directory's path to create\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    for d in directory:\n",
    "        if os.path.isdir(d):\n",
    "            print(\"directory {} already exist\".format(d))\n",
    "        if os.path.isdir(d)==False:\n",
    "            os.mkdir(path=d)\n",
    "            print(\"directory {} created successfully\".format(d))\n",
    "\n",
    "def delete_directory(directorys):\n",
    "    \"\"\"\n",
    "    Delete directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directory's path to delete along with all the files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(directorys)>=1:\n",
    "        for d in directorys:\n",
    "            if os.path.isdir(d):\n",
    "                try:\n",
    "                    if os.path.isfile(d):\n",
    "                        os.remove(path=d)\n",
    "                    else:\n",
    "                        shutil.rmtree(path=d)\n",
    "                        print(\"Removed: {}\".format(d))\n",
    "                except:\n",
    "                    print(\"Failed to removed: {}\".format(d))\n",
    "            else:\n",
    "                print(\"Failed to removed: {}\".format(d))\n",
    "\n",
    "def make_archive(base_name,root_dir,zip_format='zip'):\n",
    "    \"\"\"\n",
    "    Creates zip for given folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_name : name of zip file\n",
    "    root_dir : directory to archive/zip\n",
    "    zip_format : zip or tar\n",
    "        DESCRIPTION. The default is 'zip'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    shutil.make_archive(base_name=base_name, format=zip_format, root_dir=root_dir)\n",
    "\n",
    "def create_folders():\n",
    "    '''\n",
    "        Descrption: this will remove(if present previously) and create folders needed\n",
    "                    to store the data , model and label for ease access\n",
    "\n",
    "        input_parameters: None\n",
    "        return_parameters: returns the path of the data , model , label , report, zip directory\n",
    "    '''\n",
    "\n",
    "    data_path=os.path.join(os.getcwd(),\"data\")\n",
    "    model_path=os.path.join(os.getcwd(),\"model\")\n",
    "    label_path=os.path.join(os.getcwd(),\"minmax\")\n",
    "    encrypt_model_path=os.path.join(os.getcwd(),\"encrypt_model\")\n",
    "\n",
    "    report_path=os.path.join(os.getcwd(),\"reports\")\n",
    "    sample_data = os.path.join(report_path, \"sample_data\")\n",
    "    #Create Zip Path which contains data , model and label zip files\n",
    "    zip_path=os.path.join(os.getcwd(),\"zip\")\n",
    "\n",
    "    #deleting previously generated folders\n",
    "    delete_directory(directorys=[data_path,model_path,label_path,encrypt_model_path,report_path,zip_path, sample_data])\n",
    "\n",
    "    #creating folders\n",
    "    make_directory([data_path,model_path,label_path,report_path,encrypt_model_path,zip_path, sample_data])\n",
    "\n",
    "    return data_path, model_path, label_path,encrypt_model_path,report_path, zip_path, sample_data\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Description: Create model architecture\n",
    "    input_parameters: num_classes : number of classes\n",
    "    return_parameters: model\n",
    "    \"\"\"\n",
    "    XG = XGBRegressor(n_estimators=1000)\n",
    "    return XG\n",
    "\n",
    "def prepare_min_max(df):\n",
    "\n",
    "    \"\"\"Prepare MinMax\"\"\"\n",
    "\n",
    "    min_values = df.min().to_numpy()\n",
    "    max_values = df.max().to_numpy()\n",
    "    x= np.array([min_values,max_values])\n",
    "    df_m = pd.DataFrame(x,columns=df.columns)\n",
    "    df_m.to_csv(minmax_path+\"/minmax.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "def RMSE_score(model, test_x, test_y):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: to get root mean squared error of the model\n",
    "    \"\"\"\n",
    "\n",
    "    pred_y = model.predict(test_x)\n",
    "    return np.sqrt(mean_squared_error(pred_y, test_y))\n",
    "\n",
    "\n",
    "\n",
    "def training(parameter_payload:dict):\n",
    "\n",
    "    X_train = parameter_payload['X_train']\n",
    "    y_train = parameter_payload['y_train']\n",
    "    X_test = parameter_payload['X_test']\n",
    "    y_test = parameter_payload['y_test']\n",
    "\n",
    "\n",
    "    X_train = X_train[:,:,0]\n",
    "    X_test = X_test[:,:,0]\n",
    "\n",
    "    #Creating the model\n",
    "    model = create_model()\n",
    "\n",
    "    \"\"\"\n",
    "    Description: Trainig the model\n",
    "    \"\"\"\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=5,\n",
    "       verbose=True)\n",
    "\n",
    "    \"\"\"\n",
    "    Saving the model under model directory\n",
    "    \"\"\"\n",
    "    modelname = parameter_payload['model_name']\n",
    "    pickle.dump(model, open(os.path.join(parameter_payload['model_path'], modelname), 'wb'))\n",
    "\n",
    "    return model, X_test, y_test\n",
    "\n",
    "\n",
    "def encrypt_model(model, model_framework,pyc_model_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: Create encrypted Model\n",
    "    \"\"\"\n",
    "\n",
    "    if model_framework.lower() == \"scikit-learn\":\n",
    "\n",
    "        python_code ='''\n",
    "#importing libraries\n",
    "import pickle\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "    class for base model\n",
    "\"\"\"\n",
    "#define class\n",
    "class BaseModel():\n",
    "    def __init__(self,model_path=\"\"):\n",
    "\n",
    "        \"\"\"\n",
    "        constructor for class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : TYPE, optional\n",
    "            DESCRIPTION. The default is (100,1).\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def load_protected_pickleModel(self,filename,password,picklemodelname):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        model architecture\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : string\n",
    "            DESCRIPTION.zipped protected filename\n",
    "\n",
    "        password : String\n",
    "            DESCRIPTION.zipped file password\n",
    "\n",
    "        picklemodelname  :String\n",
    "            DESCRIPTION.pickle file name present in a ziiped protected\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : model\n",
    "            DESCRIPTION.\n",
    "\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(self.model_path,filename)\n",
    "        #print(filepath)\n",
    "        #print(filename)\n",
    "        with zipfile.ZipFile(filepath, 'r') as file:\n",
    "            with file.open(picklemodelname,'r',pwd = bytes(password, 'utf-8')) as f:\n",
    "                pck = pickle.load(f)\n",
    "        return pck\n",
    "\n",
    "    def predict(self,X):\n",
    "\n",
    "        \"\"\"\n",
    "        predict for given data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            DESCRIPTION.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : numpy array\n",
    "            DESCRIPTION.\n",
    "\n",
    "        \"\"\"\n",
    "        filename = \"modelencrypt.zip\"\n",
    "        password = \"987654321\"\n",
    "        picklemodelname = \"xgboost_tsf_mdl.pkl\"\n",
    "        model = self.load_protected_pickleModel(filename=filename,password=password,picklemodelname=picklemodelname)\n",
    "        pred = model.predict(X)\n",
    "        return pred'''\n",
    "        # Writing to file\n",
    "        with open(\"base_model.py\", \"w\") as file:\n",
    "            # Writing data to a file\n",
    "            file.writelines(python_code)\n",
    "\n",
    "        \"\"\"\n",
    "        Description: function to create .pyc file\n",
    "        \"\"\"\n",
    "        py_compile.compile(file=\"base_model.py\",cfile=pyc_model_path+'/base_model.pyc')\n",
    "\n",
    "        \"\"\"\n",
    "        Description: function to create zipped password protected pickle model file\n",
    "        \"\"\"\n",
    "\n",
    "        def zip_model(input_path,output_path,password,com_lvl=5):\n",
    "            pyminizip.compress(input_path, None, output_path,\n",
    "                               password, com_lvl)\n",
    "\n",
    "        zip_model(input_path =os.path.join(model_path,modelname),output_path = pyc_model_path+\"/modelencrypt.zip\" ,password = \"987654321\",com_lvl=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import tqdm\n",
    "    from time import sleep\n",
    "\n",
    "    user_workflow = {\n",
    "                 '1' : 'Loading the Dataset',\n",
    "                 '2' : 'Model Development and Training',\n",
    "                 '3' : 'Preparing Data, Model and Label',\n",
    "                 '4' : 'AIShield API Call'\n",
    "\n",
    "           }\n",
    "    if run_code:\n",
    "        steps_tqdm = tqdm.tqdm(range(1,len(user_workflow)+1))\n",
    "        for step in steps_tqdm:\n",
    "\n",
    "            if step == 1 :\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                zipped_file_path = \"PJME_hourly.csv.zip\"\n",
    "                file_name = 'PJME_hourly.csv'\n",
    "                df = load_data(zipped_file_path, file_name)\n",
    "\n",
    "\n",
    "            elif step == 2:\n",
    "                data_path, model_path, minmax_path,encrypt_model_path,report_path, zip_path, sample_data = create_folders()\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                X_train, X_test, y_train, y_test, df_test = preprocess_data(df)\n",
    "\n",
    "                training_parameters = {'X_train' : X_train ,\n",
    "                              'y_train':y_train ,\n",
    "                              'X_test' : X_test ,\n",
    "                              'y_test' : y_test,\n",
    "                              'model_path' : model_path ,\n",
    "                              'model_name' : 'xgboost_tsf_mdl.pkl',\n",
    "\n",
    "                             }\n",
    "\n",
    "                model, X_test, y_test = training(training_parameters)\n",
    "                print('RMSE score of the Decision Tree model is {}'.format(RMSE_score(model, X_test, y_test)))\n",
    "\n",
    "            elif step == 3:\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "\n",
    "                #Zip Data\n",
    "                df_test.to_csv(os.path.join(data_path,\"dataset.csv\"),index=False)\n",
    "                make_archive(base_name=os.path.join(zip_path,\"data\"),root_dir=data_path,zip_format='zip')\n",
    "\n",
    "                #Zip Minmax\n",
    "                prepare_min_max(df_test)\n",
    "                make_archive(base_name=os.path.join(zip_path,\"minmax\"),root_dir=minmax_path,zip_format='zip')\n",
    "\n",
    "                #Creating PYC File\n",
    "                model_framework = \"scikit-learn\"\n",
    "                model_encryption = 0 #0 for non encrypted file and 1 for encrypted (pyc)\n",
    "\n",
    "                #Description: Zip Model\n",
    "                if os.path.isfile(os.path.join(zip_path,\"model.zip\")):\n",
    "                    delete_directory(directorys=[os.path.join(zip_path,\"model.zip\")])\n",
    "                if model_encryption:\n",
    "                    encrypt_model(model,model_framework, encrypt_model_path)\n",
    "                    make_archive(base_name=os.path.join(zip_path,\"encrypt_model\"),root_dir=encrypt_model_path,zip_format='zip')\n",
    "                else:\n",
    "                    make_archive(base_name=os.path.join(zip_path,\"model\"),root_dir=model_path,zip_format='zip')\n",
    "\n",
    "            elif step == 4:\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                print(\"Below Cells will see How to integrate and Call AIShield for Vulnerability Analysis\")\n",
    "                break\n",
    "\n",
    "            sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097daf8f",
   "metadata": {
    "id": "097daf8f"
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db975542",
   "metadata": {
    "id": "db975542"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/wk2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983a3c9",
   "metadata": {
    "id": "7983a3c9"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d999b",
   "metadata": {
    "id": "762d999b"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d564a0",
   "metadata": {
    "id": "a8d564a0"
   },
   "source": [
    "## 3.0 Vulnerability Analysis by AIShield\n",
    "\n",
    "**STEP 5 : Before initiating the Vulnerability Analysis conducted by AIShield, it is recommended to go through the sample dasboard images of an completed vulnearbility analysis and sample reports <span style=\"color:brown\">(embeded in next code cell/blocks below)</span> for both Vulnerability Assessment and Defense. These reports offer valuable insights into the structure, content, and outcomes of the analysis.**\n",
    "\n",
    "By examining these reports, we can gain a comprehensive understanding of the assessment process, including the methodology employed and the valuable insights it provides. This will help us prepare effectively for the upcoming Vulnerability Analysis conducted by AIShield.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f5f5f",
   "metadata": {},
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/dashboard_images/Dashboard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afd879",
   "metadata": {},
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/dashboard_images/TSF_MEA_ML_Dashboard.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fe019d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "e8fe019d",
    "outputId": "d4423096-6f5a-4244-b22e-1e799942be4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<embed src=\"https://aisdocs.blob.core.windows.net/reference/Reports/TimeSeries/Forecasting/MEA/ML/VulnerabilityReport.pdf\" width=\"100%\" height=\"600px\" type=\"application/pdf\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hosted_pdf_url = \"https://aisdocs.blob.core.windows.net/reference/Reports/TimeSeries/Forecasting/MEA/ML/VulnerabilityReport.pdf\"\n",
    "display_pdf( output_filename = \"vul_sample\",runtime_environment = runtime_environment,pdf_path =hosted_pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e297c568",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "e297c568",
    "outputId": "82150261-ef74-47ef-823d-fed1eebd7f11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<embed src=\"https://aisdocs.blob.core.windows.net/reference/Reports/TimeSeries/Forecasting/MEA/ML/DefenceReport.pdf\" width=\"100%\" height=\"600px\" type=\"application/pdf\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hosted_defense_pdf_url = 'https://aisdocs.blob.core.windows.net/reference/Reports/TimeSeries/Forecasting/MEA/ML/DefenceReport.pdf'\n",
    "display_pdf(output_filename = \"def_sample\", runtime_environment=runtime_environment, pdf_path =hosted_defense_pdf_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260f101",
   "metadata": {
    "id": "b260f101"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25992a7e",
   "metadata": {
    "id": "25992a7e"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e488be8",
   "metadata": {
    "id": "9e488be8"
   },
   "source": [
    "#### Let's proceed with the step-by-step process of calling and integrating AIShield for Vulnerability Analysis:\n",
    "\n",
    "1. **Initialize AIShield:** Ensure that AIShield is properly installed and its dependencies are set up. Also, initialize AIShield to prepare for the analysis.\n",
    "\n",
    "2. **Model Registration:** Register the specific task pair (e.g. image_classification) and analysis type (e.g. Extraction / MEA) to perform the vulnerability analysis accurately.\n",
    "\n",
    "3. **Upload Artifacts:** Upload the necessary artifacts that have been prepared previously in STEP 4 for analysis. These may include datasets, trained models, or any relevant files.\n",
    "\n",
    "4. **Model Analysis:** Trigger the model analysis API to initiate the vulnerability analysis assessment. AIShield will perform the analysis using the uploaded artifacts.\n",
    "\n",
    "5. **Monitor Analysis Status:** Keep track of the analysis progress and patiently wait for AIShield to generate the vulnerability assessment results. This may take some time depending on the complexity of the analysis.\n",
    "\n",
    "6. **Download Reports & Artifacts:** Once the analysis is complete, access and download the vulnerability analysis reports and any additional artifacts generated by AIShield. Analyze these reports to gain insights into potential vulnerabilities in your system or application.\n",
    "\n",
    "By following instructions, you can effectively call and integrate AIShield for Vulnerability Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1c524",
   "metadata": {
    "id": "39a1c524"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fcb07",
   "metadata": {
    "id": "fe5fcb07"
   },
   "source": [
    "### 3.1 Initialize AIShield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5bd6214",
   "metadata": {
    "id": "d5bd6214"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : AIShield URL , subscription key and orgid\n",
    "              Initialize the\n",
    "\"\"\"\n",
    "base_url = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "url=base_url+\"/api/ais/v1.5\"\n",
    "org_id = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Description: Initialize the AIShield API\n",
    "\"\"\"\n",
    "\n",
    "if pypi:\n",
    "    aishield_client = ais.AIShieldApi(api_url=url,org_id=org_id)\n",
    "\n",
    "else:\n",
    "    def get_aws_api_key(url, org_id):\n",
    "\n",
    "        \"\"\"\n",
    "        Description: to get the x_api_key\n",
    "        \"\"\"\n",
    "        url = url+\"/get_aws_api_key\"\n",
    "        headers = {'org_id': org_id}\n",
    "        payload = {}\n",
    "\n",
    "        x_api_key_request = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "        status_code = x_api_key_request.status_code\n",
    "        x_api_key_request = json.loads(x_api_key_request.text)\n",
    "\n",
    "\n",
    "        if status_code == 200:\n",
    "            x_api_key = x_api_key_request['x_api_key']\n",
    "            return x_api_key\n",
    "\n",
    "        else:\n",
    "            print(x_api_key_request)\n",
    "            return None\n",
    "\n",
    "    x_api_key = get_aws_api_key(url, org_id)\n",
    "    print(\"x_api_key is :\",x_api_key)\n",
    "\n",
    "    headers={'Cache-Control': 'no-cache',\n",
    "    'x-api-key': x_api_key,\n",
    "    'Org-Id' : org_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43a3f5",
   "metadata": {
    "id": "5e43a3f5"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acfb3c",
   "metadata": {
    "id": "f8acfb3c"
   },
   "source": [
    "### 3.2 Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0566eb71",
   "metadata": {
    "id": "0566eb71"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Define the task and analysis type for model registration\n",
    "\"\"\"\n",
    "\n",
    "if pypi:\n",
    "    task_type = ais.get_type(\"task\", \"timeseries_forecasting\")\n",
    "    analysis_type = ais.get_type(\"analysis\", \"extraction\")\n",
    "\n",
    "\n",
    "    #Perform model registration\n",
    "    try:\n",
    "        status, model_registration_repsone = aishield_client.register_model(task_type=task_type, analysis_type=analysis_type)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "else:\n",
    "    \"\"\"\n",
    "    Description: call Model registration api to get unique model it and url to upload data, model and label\n",
    "    \"\"\"\n",
    "    model_registration_url = url + \"/model_registration/upload\"\n",
    "    model_registration_payload = {\n",
    "        'task_type':\"timeseries_forecasting\",\n",
    "        \"analysis_type\": \"extraction\"\n",
    "    }\n",
    "\n",
    "    new_request = requests.request(method=\"POST\", url=model_registration_url, headers=headers, json=model_registration_payload)\n",
    "    status_cd = new_request.status_code\n",
    "    new_request = json.loads(new_request.text)\n",
    "    try:\n",
    "        if status_cd == 200:\n",
    "            model_id = new_request['data']['model_id']\n",
    "            data_upload_obj = new_request['data']['urls']['data_upload_url']\n",
    "            minmax_upload_obj = new_request['data']['urls']['minmax_upload_url']\n",
    "            model_upload_obj = new_request['data']['urls']['model_upload_url']\n",
    "            print('model_id: ', model_id)\n",
    "        else:\n",
    "            print(new_request)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(new_request, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc66723",
   "metadata": {
    "id": "fcc66723"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9af44f",
   "metadata": {
    "id": "9d9af44f"
   },
   "source": [
    "### 3.3 Upload input artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b14e2ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b14e2ea",
    "outputId": "830b4113-f21e-4de9-e4e9-094cd082fb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload status: data file upload successful, minmax file upload successful, model file upload successful\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Full File paths and upload input artifacts\n",
    "\"\"\"\n",
    "zip_path = 'zip/'\n",
    "data_path=os.path.join(zip_path,'data.zip') #full path of data zip\n",
    "minmax_path=os.path.join(zip_path,'minmax.zip') #full path of label zip\n",
    "model_path=os.path.join(zip_path,'model.zip') #full path of model zip\n",
    "# model_path=os.path.join(zip_path,'encrypt_model.zip') #full path of model zip, uncomment if if model encryption is 1\n",
    "\n",
    "\n",
    "if pypi:\n",
    "    #upload input artifacts\n",
    "    upload_status = aishield_client.upload_input_artifacts(job_details=model_registration_repsone,\n",
    "                                                  data_path=data_path,\n",
    "                                                  minmax_path=minmax_path,\n",
    "                                                  model_path=model_path, )\n",
    "    print('Upload status: {}'.format(', '.join(upload_status)))\n",
    "\n",
    "else:\n",
    "    def upload_file(upload_obj, file_path):\n",
    "\n",
    "        url = upload_obj['url']\n",
    "        request_payload = upload_obj['fields']\n",
    "        files=[\n",
    "                  ('file',(os.path.basename(file_path),open(file_path,'rb'),'application/zip'))\n",
    "                ]\n",
    "\n",
    "        headers = {}\n",
    "        new_request = requests.request(\"POST\", url, headers=headers, data=request_payload, files=files)\n",
    "        status_cd = new_request.status_code\n",
    "        if status_cd == 204:\n",
    "            status = 'upload sucessful'\n",
    "        else:\n",
    "            status = 'upload failed'\n",
    "        return status\n",
    "\n",
    "    \"\"\"\n",
    "    Description: Hit AIShield File Upload API\n",
    "    \"\"\"\n",
    "    data_upload_status = upload_file(data_upload_obj, data_path)\n",
    "    minmax_upload_status = upload_file(minmax_upload_obj, minmax_path)\n",
    "    model_upload_status = upload_file(model_upload_obj, model_path)\n",
    "    print('data_upload_status: ', data_upload_status)\n",
    "    print('minmax_upload_status: ', minmax_upload_status)\n",
    "    print('model_upload_status: ', model_upload_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72300a7c",
   "metadata": {
    "id": "72300a7c"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5253322",
   "metadata": {
    "id": "a5253322"
   },
   "source": [
    "### 3.4 Model Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db84a04a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db84a04a",
    "outputId": "0993b15a-f9ad-4b6a-b848-e7e11d979424"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: Specify the appropriate configs required for vulnerability analysis and trigger model analysis\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Specify the appropriate configs required for vulnerability analysis and trigger model analysis\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if pypi:\n",
    "    vuln_config = ais.VulnConfig(task_type=task_type,\n",
    "                                 analysis_type=analysis_type,\n",
    "                                 defense_generate=True)\n",
    "\n",
    "    vuln_config.input_dimensions = input_shape # input dimension of banking marketing dataset\n",
    "    vuln_config.encryption_strategy = model_encryption  # value 0 (or) 1, if model is unencrypted or encrypted(pyc) respectively\n",
    "    vuln_config.model_framework = \"scikit-learn\"\n",
    "    vuln_config.vulnerability_threshold = 0\n",
    "    vuln_config.attack_type = \"blackbox\"\n",
    "    vuln_config.number_of_attack_queries = 10000\n",
    "\n",
    "    print('TSF-Extraction parameters are: \\n {} '.format(vuln_config.get_all_params()))\n",
    "\n",
    "    #Run vulnerability analysis\n",
    "    job_status, job_details = aishield_client.vuln_analysis(model_id=model_registration_repsone.model_id, vuln_config=vuln_config)\n",
    "\n",
    "    #unique job_id for the analyis\n",
    "    job_id = job_details.job_id\n",
    "    print('\\nstatus: {} \\nJob_id: {} \\n'.format(job_status, job_id))\n",
    "\n",
    "    #Monitor progress for given Job ID using the Link below\n",
    "    print('Click on the URL to view Vulnerability Dashboard (GUI): {}'.format(job_details.job_dashboard_uri))\n",
    "\n",
    "else:\n",
    "\n",
    "    \"\"\"\n",
    "    Description: Payload for AIShield VulnerabilityReport api call\n",
    "    \"\"\"\n",
    "    payload={}\n",
    "    payload['use_model_api']=\"no\"\n",
    "    payload['model_api_details']=\"no\"\n",
    "    payload['input_dimensions']=str(input_shape)\n",
    "    payload['model_framework']='scikit-learn'\n",
    "    payload['defense_bestonly']=\"no\"\n",
    "    payload['encryption_strategy']= model_encryption\n",
    "    payload['attack_type'] = \"blackbox\"\n",
    "    payload['number_of_attack_queries'] = 10000\n",
    "    payload['vulnerability_threshold'] = 0\n",
    "\n",
    "    print(\"TSF-Extraction parameters are:\",payload)\n",
    "    \"\"\"\n",
    "    Description: Hit AIShield VulnerabilityReport api\n",
    "    \"\"\"\n",
    "    model_analysis_url = url + \"/model_analyse/{}\".format(model_id)\n",
    "    if data_upload_status == \"upload sucessful\" and model_upload_status == \"upload sucessful\" and minmax_upload_status == \"upload sucessful\":\n",
    "        new_request = requests.request(method=\"POST\", url=model_analysis_url, json=payload,headers=headers)\n",
    "        new_request=json.loads(new_request.text)\n",
    "        for k, v in new_request.items():\n",
    "            print(\"* {} : {}\".format(k,v))\n",
    "\n",
    "        job_id=new_request['job_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607ee62",
   "metadata": {
    "id": "b607ee62"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1214a5b",
   "metadata": {
    "id": "d1214a5b"
   },
   "source": [
    "### 3.5 Monitor Analysis Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d53027b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d53027b",
    "outputId": "eaf95ccb-14f2-41ce-95bc-35a6888eb8e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: Fetch Job status using Job ID\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Fetch Job status using Job ID\n",
    "\"\"\"\n",
    "\n",
    "if pypi:\n",
    "    status = aishield_client.job_status (job_id = job_id)\n",
    "    print('job status ', status)\n",
    "\n",
    "else:\n",
    "    def monitor_api_progress(new_job_id):\n",
    "        job_status_url = url + \"/job_status_detailed?job_id=\" + new_job_id\n",
    "\n",
    "        # status dictionary\n",
    "        status_dictionary = {\n",
    "            'ModelExploration_Status': 'na',\n",
    "            'SanityCheck_Status': 'na',\n",
    "            'QueryGenerator_Status': 'na',\n",
    "            'VunerabilityEngine_Status': 'na',\n",
    "            'DefenseReport_Status': 'na',\n",
    "        }\n",
    "        counts = [0] * len(status_dictionary)\n",
    "        failed_api_hit_count = 0\n",
    "        while True:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                job_status_response = requests.request(\"GET\", job_status_url, params={},\n",
    "                                                       headers=headers)\n",
    "\n",
    "                job_status_payload = json.loads(job_status_response.text)\n",
    "                failing_key = 'ModelExploration_Status'\n",
    "                for i, key in enumerate(status_dictionary.keys()):\n",
    "                    if status_dictionary[key] == 'na':\n",
    "                        if job_status_payload[key] == 'inprogress' and status_dictionary[key] == 'na':\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                        elif job_status_payload[key] == 'completed' or job_status_payload[key] == 'passed':\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            counts[i] += 1\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                        if job_status_payload[key] == 'failed':\n",
    "                            failing_key = key\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                    elif job_status_payload[key] == 'completed' or job_status_payload[key] == 'passed':\n",
    "                        status_dictionary[key] = job_status_payload[key]\n",
    "                        if counts[i] < 1:\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "                        counts[i] += 1\n",
    "\n",
    "                    else:\n",
    "                        if job_status_payload[key] == 'failed':\n",
    "                            failing_key = key\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                if job_status_payload[failing_key] == 'failed':\n",
    "                    break\n",
    "\n",
    "                if status_dictionary['VunerabilityEngine_Status'] == 'passed' or status_dictionary[\n",
    "                    'VunerabilityEngine_Status'] == 'completed' and job_status_payload[\n",
    "                    'CurrentStatus'] == \"Defense generation is not triggered\":\n",
    "                    print(\"\\n Vulnerability score {} failed to cross vulnerability threshold of {}\".format(\n",
    "                        job_status_payload['VulnerabiltyScore'], payload['vulnerability_threshold']))\n",
    "                    break\n",
    "                if job_status_payload['DefenseReport_Status'] == 'completed':\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                failed_api_hit_count += 1\n",
    "                print(\"Error {}. trying {} ...\".format(str(e), failed_api_hit_count))\n",
    "                if failed_api_hit_count >= 3:\n",
    "                    break\n",
    "        return status_dictionary\n",
    "\n",
    "    status_dictionary = monitor_api_progress(new_job_id=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e9892",
   "metadata": {
    "id": "379e9892"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3de379",
   "metadata": {
    "id": "cb3de379"
   },
   "source": [
    "### 3.6 Download Reports and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed1442b6",
   "metadata": {
    "id": "ed1442b6"
   },
   "outputs": [],
   "source": [
    "report_path = \"reports/\"\n",
    "status =\"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "666ea4a7",
   "metadata": {
    "id": "666ea4a7"
   },
   "outputs": [],
   "source": [
    "def download_artifact(job_id, report_path, report_type='Vulnerability', file_format=0):\n",
    "    \"\"\"\n",
    "    job_id: job_id  received after successful api call\n",
    "    report_type: report to be downloaded\n",
    "    file_format: change file_format to : 0- all report in zip\n",
    "                        1- report in .txt\n",
    "                        2- report in .pdf\n",
    "                        3- report in .json\n",
    "                        4- report in .xml\n",
    "    \"\"\"\n",
    "    print(\"received report_type : {} and file format is: {}\".format(report_type, file_format))\n",
    "    report_url = url + \"/\" + \"get_report?job_id=\" + str(\n",
    "        job_id) + \"&report_type=\" + report_type + \"&file_format=\" + str(file_format)\n",
    "\n",
    "    headers1 = headers\n",
    "    headers1[\"content-type\"] = \"application/zip\"\n",
    "\n",
    "    response = requests.request(\"GET\", report_url, params={}, headers=headers1)\n",
    "\n",
    "    file_path = None\n",
    "    if file_format == 0 or file_format == \"Attack_samples\":\n",
    "        file_path=os.path.join(report_path, report_type + \".zip\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 1:\n",
    "        file_path=os.path.join(report_path, report_type + \".txt\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 2:\n",
    "        file_path=os.path.join(report_path, report_type + \".pdf\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 3:\n",
    "        file_path=os.path.join(report_path, report_type + \".json\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 4:\n",
    "        file_path=os.path.join(report_path, report_type + \".xml\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d25607b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d25607b8",
    "outputId": "397c8460-2413-43d1-bb49-8502c3bbf35f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 10:28:55,162 - INFO - directory reports/ already exist\n",
      "INFO:aishield.utils.util:directory reports/ already exist\n",
      "2023-07-13 10:28:56,976 - INFO - vulnerability_20230713_1028.pdf is saved in reports/\n",
      "INFO:aishield.connection:vulnerability_20230713_1028.pdf is saved in reports/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Defense Reports\n",
    "\"\"\"\n",
    "\n",
    "if pypi:\n",
    "    if status == \"success\":\n",
    "        output_conf = ais.OutputConf(report_type=ais.get_type(\"report\", \"vulnerability\"),\n",
    "                                     file_format=ais.get_type(\"file_format\", \"pdf\"),\n",
    "                                     save_folder_path=report_path)\n",
    "\n",
    "        vul_report = aishield_client.save_job_report(job_id=job_id, output_config=output_conf)\n",
    "\n",
    "else:\n",
    "\n",
    "    vul_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Vulnerability', file_format=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "138c15be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "138c15be",
    "outputId": "3ad8b4d6-1fc6-4890-fc97-51e959875555"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 10:28:56,994 - INFO - directory reports/ already exist\n",
      "INFO:aishield.utils.util:directory reports/ already exist\n",
      "2023-07-13 10:28:58,752 - INFO - defense_20230713_1028.pdf is saved in reports/\n",
      "INFO:aishield.connection:defense_20230713_1028.pdf is saved in reports/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Defense Reports\n",
    "\"\"\"\n",
    "if pypi:\n",
    "    if status == \"success\":\n",
    "        output_conf = ais.OutputConf(report_type=ais.get_type(\"report\", \"defense\"),\n",
    "                                     file_format=ais.get_type(\"file_format\", \"pdf\"),\n",
    "                                     save_folder_path=report_path)\n",
    "\n",
    "        def_report = aishield_client.save_job_report(job_id=job_id, output_config=output_conf)\n",
    "else:\n",
    "    def_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Defense', file_format=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8fd5876",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8fd5876",
    "outputId": "00fd303d-24af-408b-ec58-6149e5e49c80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 10:28:58,780 - INFO - directory reports/ already exist\n",
      "INFO:aishield.utils.util:directory reports/ already exist\n",
      "2023-07-13 10:28:59,525 - INFO - defense_artifact_20230713_1028.zip is saved in reports/\n",
      "INFO:aishield.connection:defense_artifact_20230713_1028.zip is saved in reports/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Defense artifacts: Model\n",
    "\"\"\"\n",
    "\n",
    "if pypi:\n",
    "    if status == \"success\":\n",
    "        output_conf = ais.OutputConf(report_type=ais.get_type(\"report\", \"defense_artifact\"),\n",
    "                                     file_format=ais.get_type(\"file_format\", \"pdf\"),\n",
    "                                     save_folder_path=report_path)\n",
    "\n",
    "        def_artifact_report = aishield_client.save_job_report(job_id=job_id, output_config=output_conf)\n",
    "else:\n",
    "    def_artifact_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Defense_artifact', file_format=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f72df282",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f72df282",
    "outputId": "3095677f-4aeb-4baf-e0a8-32a497a423c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 10:28:59,550 - INFO - directory reports/ already exist\n",
      "INFO:aishield.utils.util:directory reports/ already exist\n",
      "2023-07-13 10:29:00,060 - INFO - attack_samples_20230713_1029.zip is saved in reports/\n",
      "INFO:aishield.connection:attack_samples_20230713_1029.zip is saved in reports/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Attack Samples\n",
    "\"\"\"\n",
    "if pypi:\n",
    "    if status == \"success\":\n",
    "        output_conf = ais.OutputConf(report_type=ais.get_type(\"report\", \"attack_samples\"),\n",
    "                                     save_folder_path=report_path)\n",
    "\n",
    "        attack_report = aishield_client.save_job_report(job_id=job_id, output_config=output_conf)\n",
    "else:\n",
    "    attack_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Attack_samples', file_format=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d638a29b",
   "metadata": {
    "id": "d638a29b"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_file_id(report_path):\n",
    "\n",
    "    file_id = None\n",
    "    !pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    import os\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "    # Authenticate and authorize access to the Google Drive API\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "    # Build the Drive API service\n",
    "    drive_service = build('drive', 'v3')\n",
    "\n",
    "    # Path to the PDF file in your local drive\n",
    "#         local_pdf_path = '/content/drive/MyDrive/Vulnerability.pdf'\n",
    "\n",
    "    # Upload the PDF file to Google Drive\n",
    "    pdf_file_name = os.path.basename(report_path)\n",
    "    file_metadata = {'name': pdf_file_name}\n",
    "    media = MediaFileUpload(report_path, mimetype='application/pdf')\n",
    "    file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    # Set sharing permissions to \"Anyone with the link\"\n",
    "    file_id = file.get('id')\n",
    "    drive_service.permissions().create(\n",
    "        fileId=file_id,\n",
    "        body={'role': 'reader', 'type': 'anyone'}\n",
    "    ).execute()\n",
    "\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e297509",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0e297509",
    "outputId": "74b1d921-3426-47fb-95de-b42de3c61f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.93.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.21.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.59.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.5.7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/1_xuI-lMEWmotF1LhniXbSEh1FY46JvqJ/preview\" width=\"100%\" height=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Displaying the current Vulnerability Assement report\n",
    "\"\"\"\n",
    "\n",
    "if (\"colab\" in runtime_environment.lower()):\n",
    "    file_id = get_file_id(report_path = vul_report)\n",
    "    display_pdf(output_filename = \"vul_\"+job_id[-10:],runtime_environment=runtime_environment,file_id=file_id)\n",
    "else:\n",
    "    display_pdf(output_filename = \"vul_\"+job_id[-10:],runtime_environment=runtime_environment, pdf_path =os.path.join(report_path, os.path.basename(vul_report)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf68cd81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bf68cd81",
    "outputId": "27c22255-1f1c-4ac3-985c-7e88558c5f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.93.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.21.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.59.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.5.7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/1Iso3wPWQe92NvLmUw3xuk120C-464UaX/preview\" width=\"100%\" height=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Displaying the current Defense Assement report\n",
    "\"\"\"\n",
    "\n",
    "if (\"colab\" in runtime_environment.lower()):\n",
    "    file_id = get_file_id(report_path = def_report)\n",
    "    display_pdf(output_filename = \"Defense_\"+job_id[-10:],runtime_environment=runtime_environment,file_id=file_id)\n",
    "else:\n",
    "    display_pdf(output_filename = \"Defense_\"+job_id[-10:],runtime_environment=runtime_environment, pdf_path =os.path.join(report_path, os.path.basename(def_report)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b657cb6",
   "metadata": {
    "id": "3b657cb6"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw6.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a47047",
   "metadata": {
    "id": "67a47047"
   },
   "source": [
    "### After conducting a Vulnerability Analysis by AIshield, if a defense recommendation is identified, then following actions can be taken:\n",
    "\n",
    "1. **Model Retraining:** It is advised to retrain the original model to enhance its robustness. By revisiting the training process and incorporating the insights gained from the vulnerability analysis, the model can be improved to better handle potential threats and mitigate vulnerabilities.\n",
    "\n",
    "\n",
    "2. **Deploying Threat-Informed Defense Model:** In addition to the original model, AIshield provides a specialized defense model that is specifically designed to address the identified threats. This threat-informed defense model can be deployed alongside the original model to provide an additional layer of protection and enhance the overall security posture. By leveraging the insights gained from the vulnerability analysis, the defense model offers targeted defenses against potential threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb6cc6b",
   "metadata": {
    "id": "9bb6cc6b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
