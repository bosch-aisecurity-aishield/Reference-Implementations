{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ce47d1",
   "metadata": {
    "id": "64ce47d1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Welcome to the AIShield Integration Notebook!\n",
    "\n",
    "**We are delighted to have you here!**\n",
    "\n",
    "## What to Expect !!\n",
    "This notebook serves as a comprehensive demonstration of the seamless integration capabilities of AIShield. As a powerful tool for securing your AI/ML assets against adversarial threats, AIShield ensures the integrity and robustness of your models, preventing financial loss, protecting brand reputation, and mitigating the risk of intellectual property theft.\n",
    "\n",
    "## What to Do !!\n",
    "- **Explore the notebook** at your own pace to gain deep insights into the functionalities and features offered by AIShield.\n",
    "- **Run the first code cell/block**, which includes **Support functionalities** crucial for the smooth execution of the notebook. This setup will enhance your experience and enable the seamless integration of AIShield.\n",
    "- Even without running the entire notebook, you can get a glimpse of AIShield's capabilities and its approach to vulnerability assessments through sample scenarios.\n",
    "- **Run the code cells** to witness firsthand how AIShield seamlessly integrates with your existing codebase, empowering you to protect your AI models against adversarial attacks.\n",
    "- **Experiment and adapt the code** to suit your specific use cases, leveraging the flexibility and customization options provided by AIShield.\n",
    "- Should you have any questions or require assistance, our dedicated [AIShield.Contact@bosch.com](mailto:AIShield.Contact@bosch.com) is just a click away.\n",
    "\n",
    "## What You'll Get !!\n",
    "Throughout this notebook, you'll find:\n",
    "- Code snippets demonstrating step-by-step procedures for incorporating AIShield into your existing workflows, ensuring comprehensive protection against adversarial threats.\n",
    "- Detailed guidance on the integration process, including any additional inputs required for thorough vulnerability analysis.\n",
    "- A comprehensive threat-informed defense model, empowering you to understand the vulnerabilities in your AI models and undertake appropriate remediation measures.\n",
    "- Report artifacts and a sample attack dataset, providing valuable insights into potential attack scenarios and assisting in devising robust defense strategies.\n",
    "- In-depth explanations of defense remediation techniques, equipping you with the knowledge and tools to safeguard your AI assets effectively.\n",
    "\n",
    "## <span style=\"color:teal\">AIShield Website and LinkedIn</span>\n",
    "To learn more about AIShield and its cutting-edge features, visit the official [AIShield website](https://www.boschaishield.com/). Connect with AIShield on [LinkedIn](https://www.linkedin.com/company/bosch-aishield/about/) to stay updated on the latest advancements in AI security.\n",
    "\n",
    "We hope this notebook empowers you to seamlessly integrate AIShield into your projects, ensuring the utmost protection for your AI/ML assets. If you have any questions or need further assistance, please don't hesitate to reach out. **Happy exploring!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04dd47",
   "metadata": {
    "id": "cc04dd47",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bosch-aisecurity-aishield/Reference-Implementations/blob/main/Product_Taskpair_wise/Image_Classification_UseModelApi/Image_Classification_Extraction.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5cabeb",
   "metadata": {
    "id": "5e5cabeb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.0 Support Functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "BmmyVE_nqXTy",
   "metadata": {
    "id": "BmmyVE_nqXTy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: if you want to do the vulnerability analysis through the hosted endpoint\n",
    "             then mark it as yes or else mark it as no\n",
    "\"\"\"\n",
    "\n",
    "use_model_api = \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fb5754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "95fb5754",
    "outputId": "4c44ca52-0903-4429-de40-77eae192b597",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is running in Jupyter Notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "        .scrollable-code-input {\n",
       "            max-height: 500px;\n",
       "            overflow-y: scroll;\n",
       "        }\n",
       "        </style>\n",
       "        <script>\n",
       "        var codeInputCells = document.querySelectorAll('div.code_cell .input');\n",
       "        codeInputCells.forEach(function(cell) {\n",
       "            cell.classList.add('scrollable-code-input');\n",
       "        });\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_code_input_scrollable():\n",
    "    \"\"\"Description : To make the input code cell scrollable for jupyternotebook\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "        .scrollable-code-input {\n",
    "            max-height: 500px;\n",
    "            overflow-y: scroll;\n",
    "        }\n",
    "        </style>\n",
    "        <script>\n",
    "        var codeInputCells = document.querySelectorAll('div.code_cell .input');\n",
    "        codeInputCells.forEach(function(cell) {\n",
    "            cell.classList.add('scrollable-code-input');\n",
    "        });\n",
    "        </script>\n",
    "    '''))\n",
    "\n",
    "def display_pdf(runtime_environment,output_filename, pdf_path=None, file_id=None):\n",
    "\n",
    "    \"\"\"Descrption : To embeded pdf file in the ntoebook\"\"\"\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    import os\n",
    "    output_file = output_filename+'.html'\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            saved_output = f.read()\n",
    "            display(HTML(saved_output))\n",
    "    else:\n",
    "        if 'colab' in runtime_environment.lower():\n",
    "            # Generate the Google Drive Viewer URL\n",
    "            if file_id is None:\n",
    "                print(pdf_path)\n",
    "                # Generate the Google Drive Viewer URL\n",
    "                drive_viewer_url = f'https://drive.google.com/viewerng/viewer?embedded=true&url={pdf_path}'\n",
    "                html_code = f'<iframe src=\"{drive_viewer_url}\" width=\"100%\" height=\"600px\"></iframe>'\n",
    "            else:\n",
    "                viewer_url = f\"https://drive.google.com/file/d/{file_id}/preview\"\n",
    "                html_code = f'<iframe src=\"{viewer_url}\" width=\"100%\" height=\"600px\"></iframe>'\n",
    "        else:\n",
    "            html_code = f'<embed src=\"{pdf_path}\" width=\"100%\" height=\"600px\" type=\"application/pdf\">'\n",
    "        # Save the output to a file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(html_code)\n",
    "\n",
    "        # Display the output\n",
    "        display(HTML(html_code))\n",
    "\n",
    "\n",
    "def check_runtime_environment():\n",
    "    \"\"\"\n",
    "    Description : Check notebook is running on colab or jupyter-notebook\n",
    "    \"\"\"\n",
    "    import os\n",
    "    colab = False\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        colab = True\n",
    "        return 'Colab' , colab\n",
    "    else:\n",
    "        return 'Jupyter Notebook' , colab\n",
    "\n",
    "# Check the runtime environment\n",
    "runtime_environment,colab_running  = check_runtime_environment()\n",
    "\n",
    "print(\"Notebook is running in\", runtime_environment)\n",
    "\n",
    "make_code_input_scrollable()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a83b9",
   "metadata": {
    "id": "452a83b9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb01f37",
   "metadata": {
    "id": "bfb01f37",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137bb2f",
   "metadata": {
    "id": "e137bb2f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/wk1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb6716",
   "metadata": {
    "id": "1bcb6716",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aafb23",
   "metadata": {
    "id": "09aafb23",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f2642",
   "metadata": {
    "id": "eb1f2642",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 Below code cell/block performs Step 1 to 4 with following these sequential steps :\n",
    "**STEP 1 : Installing libraries:** The required libraries will be installed to provide necessary functionalities for the task at hand.\n",
    "\n",
    "**STEP 2 : Loading the dataset:** The dataset containing the relevant information will be loaded into the program, allowing access to the data for analysis and modeling.\n",
    "\n",
    "**STEP 3 : Model development and training:** A sophisticated model will be developed to analyze and interpret the dataset. The model will then be trained using appropriate techniques to optimize its performance.\n",
    "\n",
    "**STEP 4 : Preparing the Data, Model, and Label:** Prior to further analysis, essential steps such as data preprocessing, model configuration, and label preparation will be carried out. This will ensure that the data, model, and labels are properly structured and ready for subsequent stages needed for aishield.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b1807a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:27:59.754298Z",
     "iopub.status.busy": "2023-06-05T18:27:59.754298Z",
     "iopub.status.idle": "2023-06-05T18:28:23.490327Z",
     "shell.execute_reply": "2023-06-05T18:28:23.488117Z"
    },
    "hide_code": false,
    "id": "75b1807a",
    "outputId": "efb9bfd6-5660-49fb-a9be-92b48c114423",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started of installing all the prerequisites packages\n",
      "Collecting numpy==1.22\n",
      "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\n",
      "librosa 0.10.1 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.22.0 which is incompatible.\n",
      "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.22.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.3.4\n",
      "  Downloading matplotlib-3.3.4.tar.gz (37.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.3.4) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.3.4) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.3.4) (1.22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.3.4) (9.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.3.4) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.3.4) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.4) (1.16.0)\n",
      "Building wheels for collected packages: matplotlib\n",
      "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for matplotlib: filename=matplotlib-3.3.4-cp310-cp310-linux_x86_64.whl size=11684758 sha256=624726cf74c8bbe906d379bc625344403560ab926933b712a09b98eb01e4e977\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/c6/49/eaba6d234887d98d9c85185e2a90bd7bb77934e85eefaf317e\n",
      "Successfully built matplotlib\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\n",
      "mizani 0.9.3 requires matplotlib>=3.5.0, but you have matplotlib 3.3.4 which is incompatible.\n",
      "plotnine 0.12.4 requires matplotlib>=3.6.0, but you have matplotlib 3.3.4 which is incompatible.\n",
      "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.3.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.9.1\n",
      "  Downloading tensorflow_gpu-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.6.3)\n",
      "Collecting flatbuffers<2,>=1.12 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.59.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (3.9.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.22.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (23.2)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.16.0)\n",
      "Collecting tensorboard<2.10,>=2.9 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (0.34.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow-gpu==2.9.1)\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-gpu==2.9.1) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.17.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.2.2)\n",
      "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow-gpu\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.14.0\n",
      "    Uninstalling tensorflow-estimator-2.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.14.1\n",
      "    Uninstalling tensorboard-2.14.1:\n",
      "      Successfully uninstalled tensorboard-2.14.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.14.0 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.9.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.9.1 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.9.0 which is incompatible.\n",
      "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.0.2\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.22.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.11.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.2.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 0.13.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.0.2 which is incompatible.\n",
      "librosa 0.10.1 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.0.2\n",
      "Collecting humanfriendly==9.2\n",
      "  Downloading humanfriendly-9.2-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly\n",
      "Successfully installed humanfriendly-9.2\n",
      "Collecting tqdm==4.61.1\n",
      "  Downloading tqdm-4.61.1-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tqdm-4.61.1\n",
      "Collecting requests==2.28.0\n",
      "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0 (from requests==2.28.0)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (3.4)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests==2.28.0)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.0) (2023.7.22)\n",
      "Installing collected packages: urllib3, charset-normalizer, requests\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 0.13.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.0.2 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.9.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.9.1 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.9.0 which is incompatible.\n",
      "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "yfinance 0.2.31 requires requests>=2.31, but you have requests 2.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 requests-2.28.0 urllib3-1.26.18\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Starting installing aishield library\n",
      "Collecting aishield\n",
      "  Downloading aishield-0.1.5-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from aishield) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->aishield) (2023.7.22)\n",
      "Installing collected packages: aishield\n",
      "Successfully installed aishield-0.1.5\n",
      "Installed all the prerequisites packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "Loading the Dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Development and Training:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to removed: /content/data\n",
      "Failed to removed: /content/model\n",
      "Failed to removed: /content/label\n",
      "Failed to removed: /content/encrypt_model\n",
      "Failed to removed: /content/reports\n",
      "Failed to removed: /content/zip\n",
      "Failed to removed: /content/reports/sample_data\n",
      "directory /content/data created successfully\n",
      "directory /content/model created successfully\n",
      "directory /content/label created successfully\n",
      "directory /content/reports created successfully\n",
      "directory /content/encrypt_model created successfully\n",
      "directory /content/zip created successfully\n",
      "directory /content/reports/sample_data created successfully\n",
      "Normalization of the inputdata being started\n",
      "Normalization Done\n",
      "844/844 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9254\n",
      "Epoch 1: val_loss improved from inf to 0.04624, saving model to /content/model/mnist_model.h5\n",
      "844/844 [==============================] - 15s 7ms/step - loss: 0.2413 - accuracy: 0.9254 - val_loss: 0.0462 - val_accuracy: 0.9847\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0462 - accuracy: 0.9847\n",
      "* Loss: 0.046242568641901016 \n",
      "* Accuracy: 0.9847000241279602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Data, Model and Label:  50%|█████     | 2/4 [00:28<00:32, 16.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing and Saving Data and Label file Needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AIShield API Call:  75%|███████▌  | 3/4 [00:31<00:10, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below Cells will see How to integrate and Call AIShield for Vulnerability Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Show/Hide Code\n",
    "'''\n",
    "Descri`aption: user input about dataset information, for eg: number of classes, input_shape\n",
    "'''\n",
    "\n",
    "img_row,img_col,channel=28,28,1\n",
    "num_classes=10\n",
    "epochs = 1\n",
    "batch_size= 64\n",
    "input_shape=(img_row,img_col,channel)\n",
    "model_encryption=0 #0 if model is uploaded directly as a zip, 1 if model is encryted as .pyc and uploaded as a zip\n",
    "Normalized =  'yes'\n",
    "run_code = True\n",
    "install = True #to install the prerequisite libraries or not\n",
    "\n",
    "\n",
    "## installing libraries if not and then importing\n",
    "if install:\n",
    "    print(\"Started of installing all the prerequisites packages\")\n",
    "    !pip install numpy==1.22\n",
    "    !pip install humanfriendly==9.2\n",
    "    !pip install tqdm==4.61.1\n",
    "    !pip install requests==2.28.0\n",
    "    !pip install pandas\n",
    "\n",
    "    print(\"Starting installing aishield library\")\n",
    "    !pip install aishield\n",
    "    print(\"Installed all the prerequisites packages\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipped the installation of all the prerequisites packages\")\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import shutil\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "import aishield as ais\n",
    "\n",
    "\n",
    "def download_artifacts(url):\n",
    "    \"\"\"\n",
    "    Description :\n",
    "    This function will download the dataset from the given file_path\n",
    "    parameter:\n",
    "\n",
    "        input:\n",
    "        file_path : url of the dataset from where need to download the dataset\n",
    "\n",
    "        output: None\n",
    "    \"\"\"\n",
    "    file_path = os.path.basename(url)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "#extract content from the nested zip file\n",
    "def extract_zip(zip_file_path, extract_to_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    Description :\n",
    "    This function will extract the zipped file to a given destination folder\n",
    "\n",
    "        input:\n",
    "        zip_file_path : download zipped file path\n",
    "        extract_to_folder: destination folder to extract the content of zipped files\n",
    "\n",
    "        output: None\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all contents into the target folder\n",
    "        zip_ref.extractall(extract_to_folder)\n",
    "    zip_ref.close()\n",
    "\n",
    "\n",
    "\n",
    "def download_extract_artifcats(download_url, zipped_file_path, destination_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Description :\n",
    "    This function will trigger the download and extraction function one by one\n",
    "\n",
    "        input:\n",
    "        download_url: url from where content need to be downloaded\n",
    "        zip_file_path : download zipped file path\n",
    "        destination_path: destination folder to extract the content of zipped files\n",
    "\n",
    "        output: None\n",
    "    \"\"\"\n",
    "\n",
    "    # download the artifcats from the given url\n",
    "    try:\n",
    "        download_artifacts(download_url)\n",
    "        # extract the zipped artifacts in to given destination folder\n",
    "        extract_zip(zipped_file_path, destination_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Download / Extraction of artifacts failed {}\".format(str(e)))\n",
    "\n",
    "def make_directory(directory):\n",
    "    \"\"\"\n",
    "    Create directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directory's path to create\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    for d in directory:\n",
    "        if os.path.isdir(d):\n",
    "            print(\"directory {} already exist\".format(d))\n",
    "        if os.path.isdir(d)==False:\n",
    "            os.mkdir(path=d)\n",
    "            print(\"directory {} created successfully\".format(d))\n",
    "\n",
    "def delete_directory(directorys):\n",
    "    \"\"\"\n",
    "    Delete directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directory's path to delete along with all the files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(directorys)>=1:\n",
    "        for d in directorys:\n",
    "            if os.path.isdir(d):\n",
    "                try:\n",
    "                    if os.path.isfile(d):\n",
    "                        os.remove(path=d)\n",
    "                    else:\n",
    "                        shutil.rmtree(path=d)\n",
    "                        print(\"Removed: {}\".format(d))\n",
    "                except:\n",
    "                    print(\"Failed to removed: {}\".format(d))\n",
    "            else:\n",
    "                print(\"Failed to removed: {}\".format(d))\n",
    "\n",
    "def make_archive(base_name,root_dir,zip_format='zip'):\n",
    "    \"\"\"\n",
    "    Creates zip for given folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_name : name of zip file\n",
    "    root_dir : directory to archive/zip\n",
    "    zip_format : zip or tar\n",
    "        DESCRIPTION. The default is 'zip'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    shutil.make_archive(base_name=base_name, format=zip_format, root_dir=root_dir)\n",
    "\n",
    "\n",
    "def create_folders():\n",
    "    '''\n",
    "        Descrption: this will remove(if present previously) and create folders needed\n",
    "                    to store the data , model and label for ease access\n",
    "\n",
    "        input_parameters: None\n",
    "        return_parameters: returns the path of the data , model , label , report, zip directory\n",
    "    '''\n",
    "\n",
    "    data_path=os.path.join(os.getcwd(),\"data\")\n",
    "    label_path=os.path.join(os.getcwd(),\"label\")\n",
    "\n",
    "    report_path=os.path.join(os.getcwd(),\"reports\")\n",
    "    sample_data = os.path.join(report_path, \"sample_data\")\n",
    "    #Create Zip Path which contains data , model and label zip files\n",
    "    zip_path=os.path.join(os.getcwd(),\"zip\")\n",
    "\n",
    "    #deleting previously generated folders\n",
    "    delete_directory(directorys=[data_path,label_path,report_path,zip_path, sample_data])\n",
    "\n",
    "    #creating folders\n",
    "    make_directory([data_path,label_path,report_path,zip_path, sample_data])\n",
    "\n",
    "    return data_path,label_path,report_path, zip_path, sample_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import tqdm\n",
    "    from time import sleep\n",
    "\n",
    "    user_workflow = {\n",
    "                 '1' : 'Download the Dataset',\n",
    "                 '2' : 'Model Development',\n",
    "                 '3' : 'Preparing Data, Model as per AIShield pre-requisite',\n",
    "                 '4' : 'AIShield API Call'\n",
    "\n",
    "           }\n",
    "\n",
    "    if run_code:\n",
    "        steps_tqdm = tqdm.tqdm(range(1,len(user_workflow)+1))\n",
    "        data_path, label_path,report_path, zip_path, sample_data = create_folders()\n",
    "        for step in steps_tqdm:\n",
    "\n",
    "            # if step == 1 :\n",
    "            #     steps_tqdm.set_description(user_workflow[str(step)])\n",
    "\n",
    "            if step == 1 :\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                print(\"Downloading of the Data Started\")\n",
    "                data_url = \"https://aisdocs.blob.core.windows.net/reference/upload/Text/TextClassification/IMDB_Dataset.zip\"\n",
    "                download_extract_artifcats(data_url, os.path.basename(data_url), os.path.basename(data_path))\n",
    "                print(\"Data downloaded succesfully\")\n",
    "\n",
    "\n",
    "            elif step == 2:\n",
    "                \n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                print(\"Downloading of the Label file Started\")\n",
    "                label_url = \"https://aisdocs.blob.core.windows.net/reference/upload/Text/TextClassification/nlp_model.zip\"\n",
    "                download_extract_artifcats(label_url, os.path.basename(label_url), os.path.basename(label_path))\n",
    "                print(\"Label file downloaded succesfully\")\n",
    "\n",
    "            elif step == 3:\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "\n",
    "                #Zip Data\n",
    "                make_archive(base_name=os.path.join(zip_path,\"data\"),root_dir=data_path,zip_format='zip')\n",
    "\n",
    "                #Zip Model\n",
    "                make_archive(base_name=os.path.join(zip_path,\"label\"),root_dir=label_path,zip_format='zip')\n",
    "\n",
    "            elif step == 4:\n",
    "                steps_tqdm.set_description(user_workflow[str(step)])\n",
    "                print(\"Below Cells will see How to integrate and Call AIShield for Vulnerability Analysis\")\n",
    "                break\n",
    "\n",
    "            sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775ce40",
   "metadata": {
    "id": "9775ce40",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb7b1b",
   "metadata": {
    "id": "aecb7b1b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80782b1d",
   "metadata": {
    "id": "80782b1d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/wk2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d4789",
   "metadata": {
    "id": "d78d4789",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74404585",
   "metadata": {
    "id": "74404585",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49ea41",
   "metadata": {
    "id": "7e49ea41",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0 Vulnerability Analysis by AIShield\n",
    "\n",
    "**STEP 5 :You can move to trigger the AIShield analysis by following steps given below. If you want to preview the generated artifact by vulnerability analysis like the report and dashboard, please find below a few samples of same, all without having to trigger an actual analysis.**\n",
    "\n",
    "By examining these Reports, we can gain a comprehensive understanding of the upcoming Vulnerability Analysis conducted by AIShield, which will enable us to prepare more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c2dd7",
   "metadata": {
    "id": "5a5c2dd7"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/dashboard_images/Dashboard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475ab6a",
   "metadata": {
    "id": "6475ab6a"
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/dashboard_images/IC_MEA_Dashboard.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca157e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:28:23.772534Z",
     "iopub.status.busy": "2023-06-05T18:28:23.772534Z",
     "iopub.status.idle": "2023-06-05T18:28:23.797616Z",
     "shell.execute_reply": "2023-06-05T18:28:23.795120Z"
    },
    "hide_code": false,
    "id": "0ca157e6",
    "outputId": "5ef08477-ea5d-4cd8-eb34-ce97d31353b0",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<embed src=\"https://aisdocs.blob.core.windows.net/reference/Reports/Image/ImageClassification/MEA/VulnerabilityReport.pdf\" width=\"100%\" height=\"600px\" type=\"application/pdf\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hosted_pdf_url = 'https://aisdocs.blob.core.windows.net/reference/Reports/Image/ImageClassification/MEA/VulnerabilityReport.pdf'\n",
    "display_pdf( output_filename = \"vul_sample\",runtime_environment = runtime_environment,pdf_path =hosted_pdf_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfb0e6",
   "metadata": {
    "id": "12bfb0e6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51b0fe",
   "metadata": {
    "id": "5c51b0fe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d1110",
   "metadata": {
    "id": "015d1110",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Let's proceed with the step-by-step process of calling and integrating AIShield for Vulnerability Analysis:\n",
    "\n",
    "1. **Initialize AIShield:** Ensure that AIShield is properly installed and its dependencies are set up. Also, initialize AIShield to prepare for the analysis.\n",
    "\n",
    "2. **Model Registration:** Register the specific task pair (e.g. image_classification) and analysis type (e.g. Extraction / MEA) to perform the vulnerability analysis accurately.\n",
    "\n",
    "3. **Upload Artifacts:** Upload the necessary artifacts that have been prepared previously in STEP 4 for analysis. These may include datasets, trained models, or any relevant files.\n",
    "\n",
    "4. **Model Analysis:** Trigger the model analysis API to initiate the vulnerability analysis assessment. AIShield will perform the analysis using the uploaded artifacts.\n",
    "\n",
    "5. **Monitor Analysis Status:** Keep track of the analysis progress and patiently wait for AIShield to generate the vulnerability assessment results. This may take some time depending on the complexity of the analysis.\n",
    "\n",
    "6. **Download Reports & Artifacts:** Once the analysis is complete, access and download the vulnerability analysis reports and any additional artifacts generated by AIShield. Analyze these reports to gain insights into potential vulnerabilities in your system or application.\n",
    "\n",
    "By following instructions, you can effectively call and integrate AIShield for Vulnerability Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15872b",
   "metadata": {
    "id": "8f15872b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e41392",
   "metadata": {
    "id": "c3e41392",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Initialize AIShield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91fcac5f",
   "metadata": {
    "id": "91fcac5f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : AIShield URL , subscription key and orgid\n",
    "              Initialize the\n",
    "\"\"\"\n",
    "base_url = \"https://api.aws.boschaishield.com/prod\"\n",
    "url=base_url+\"/api/ais/v1.5\"\n",
    "org_id = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' #<<Copy Org_id mentioned in welcome mail after AIShield Subscription>>\n",
    "\n",
    "\"\"\"\n",
    "Description: Initialize the AIShield API\n",
    "\"\"\"\n",
    "\n",
    "#if using AIShield API then , need an x-api-key for accessing AIShield registration / Analysis APIs\n",
    "# below function will get insight of how to generate the x-api-key\n",
    "def get_aws_api_key(url, org_id):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: to get the x_api_key\n",
    "    \"\"\"\n",
    "    url = url+\"/get_aws_api_key\"\n",
    "    headers = {'org_id': org_id}\n",
    "    payload = {}\n",
    "\n",
    "    x_api_key_request = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    status_code = x_api_key_request.status_code\n",
    "    x_api_key_request = json.loads(x_api_key_request.text)\n",
    "\n",
    "    if status_code == 200:\n",
    "        x_api_key = x_api_key_request['x_api_key']\n",
    "        return x_api_key\n",
    "\n",
    "    else:\n",
    "        print(x_api_key_request)\n",
    "        return None\n",
    "\n",
    "#generate the x-api-key\n",
    "x_api_key = get_aws_api_key(url, org_id)\n",
    "print(\"x_api_key is :\",x_api_key)\n",
    "\n",
    "headers={'Cache-Control': 'no-cache',\n",
    "'x-api-key': x_api_key,\n",
    "'Org-Id' : org_id\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdb00a",
   "metadata": {
    "id": "81cdb00a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7914958",
   "metadata": {
    "id": "b7914958",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5db4de5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:28:23.926951Z",
     "iopub.status.busy": "2023-06-05T18:28:23.926951Z",
     "iopub.status.idle": "2023-06-05T18:28:27.401047Z",
     "shell.execute_reply": "2023-06-05T18:28:27.398910Z"
    },
    "hide_code": false,
    "id": "e5db4de5",
    "outputId": "582a4c75-1fcb-4004-8c66-c435486719d8",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Define the task and analysis type for model registration\n",
    "              \"task_type\" : refers to the specific type of task being performed, for eg : \"image_classification.\"\n",
    "              \"analysis_type\" : refers to the specific type of analysis being performed,for eg : \"extraction.\n",
    "              For more information, check out [https://docs.boschaishield.com/api-docs/lesspostgreater-model-registration#-a-xGGsdVJB3d09cBAck-]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Description: call Model registration api to get unique model it and url to upload data, model and label\n",
    "\"\"\"\n",
    "model_registration_url = url + \"/model_registration/upload\"\n",
    "\n",
    "model_registration_payload = {\n",
    "    'task_type':\"ASR\",\n",
    "    \"analysis_type\": \"evasion\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    new_request = requests.request(method=\"POST\", url=model_registration_url, headers=headers, json=model_registration_payload)\n",
    "    new_request = json.loads(new_request.text)\n",
    "    print(new_request)\n",
    "    model_id = new_request['data']['model_id']\n",
    "    data_upload_obj = new_request['data']['urls']['data_upload_url']\n",
    "    label_upload_obj = new_request['data']['urls']['label_upload_url']\n",
    "    print('model_id: ', model_id)\n",
    "\n",
    "except Exception as e:\n",
    "    print(new_request, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475dc38",
   "metadata": {
    "id": "8475dc38",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8769c47",
   "metadata": {
    "id": "c8769c47",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Upload input artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb368b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:28:27.409855Z",
     "iopub.status.busy": "2023-06-05T18:28:27.404436Z",
     "iopub.status.idle": "2023-06-05T18:28:37.206324Z",
     "shell.execute_reply": "2023-06-05T18:28:37.204157Z"
    },
    "hide_code": false,
    "id": "bcb368b4",
    "outputId": "c35c0e36-8046-444a-d808-c6c474af9eaf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_upload_status:  upload sucessful\n",
      "label_upload_status:  upload sucessful\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Full File paths and upload input artifacts\n",
    "\"\"\"\n",
    "zip_path = 'zip/'\n",
    "data_path=os.path.join(zip_path,'data.zip') #full path of data zip\n",
    "label_path=os.path.join(zip_path,'label.zip') #full path of label zip\n",
    "\n",
    "def upload_file(upload_obj, file_path):\n",
    "\n",
    "    url = upload_obj['url']\n",
    "    request_payload = upload_obj['fields']\n",
    "    files=[\n",
    "              ('file',(os.path.basename(file_path),open(file_path,'rb'),'application/zip'))\n",
    "            ]\n",
    "\n",
    "    headers = {}\n",
    "    new_request = requests.request(\"POST\", url, headers=headers, data=request_payload, files=files)\n",
    "    status_cd = new_request.status_code\n",
    "    if status_cd == 204:\n",
    "        status = 'upload sucessful'\n",
    "    else:\n",
    "        status = 'upload failed'\n",
    "    return status\n",
    "\n",
    "\"\"\"\n",
    "Description: Hit AIShield File Upload API\n",
    "\"\"\"\n",
    "data_upload_status = upload_file(data_upload_obj, data_path)\n",
    "label_upload_status = upload_file(label_upload_obj, label_path)\n",
    "if use_model_api.lower() == \"no\":\n",
    "    model_upload_status = upload_file(model_upload_obj, model_path)\n",
    "    print('model_upload_status: ', model_upload_status)\n",
    "print('data_upload_status: ', data_upload_status)\n",
    "print('label_upload_status: ', label_upload_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705377c",
   "metadata": {
    "id": "f705377c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758139d",
   "metadata": {
    "id": "0758139d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4 Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67568bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : if use_model_api is yes , \n",
    "                Needed model_hosted_endpoint_url and other info like is_ssl_cert\n",
    "                is_ssl_cert : by default value is yes if hosted with certification\n",
    "                              or else mark as no \n",
    "            or else , can skip the below three cell\n",
    "\"\"\"\n",
    "\n",
    "# model hosted endpoint url\n",
    "model_hosted_endpoint_url = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" # <<Replace XXXXXXXXXXX with endpoint url>>\n",
    "\n",
    "#by default value is yes , mark it no if model is not being hosted with ssl cert on \n",
    "is_ssl_cert = \"yes\"\n",
    "\n",
    "# if ssl cert is on , read the cert content\n",
    "# Provide the SSL certificate content required to access the model endpoint through HTTPS,\n",
    "# formatted as a string. You can open the pem file in any editor(e.g. notepad) and copy the contents and past in this field.\n",
    "if is_ssl_cert.lower() == \"yes\":\n",
    "    f = open(\"XXXXXXXXXXXX.crt\", \"r\") # <<Replace XXXXXXXXXXXXXXXXX.crt with the path of the crt file>>\n",
    "    cert_pem_contents = f.read()\n",
    "else:\n",
    "    cert_pem_contents= \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b662d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : if use_model_api is yes ,Additionally Below information also needed to access the endpoint for vulnerability analysis\n",
    "                i.e : accept_datatype , input_key , response_key , cert_pem_contents if needed\n",
    "\"\"\"\n",
    "\n",
    "# For querying the endpoint, we accept the following keys: 'array', 'base64', and 'file'.\n",
    "# The data can be in the format of an n-dim-array(np.array), base64, or file, which the deployed endpoint uses for inference\n",
    "accept_datatype = \"array\"\n",
    "\n",
    "# Specify the key name in the request payload from which the API retrieves input data for inference or prediction.\n",
    "# For instance, a model payload might look like: {\"query\": \"[0.123,0.23,0.45]\"}. Here \"query\" is the input_key.\n",
    "input_key = \"XXXXXXXXX\"  # <<replace XXXXXXXXX with the actual input_key>>\n",
    "\n",
    "# Specify the key name in the response payload where the inference or predicted results can be found.\n",
    "# For example, a model response might be: {\"results\": \"[1,2,8,9]\", \"status\": \"success\"}. Here, \"results\" is the response_key\n",
    "response_key = \"XXXXXXXXX\" # <<replace XXXXXXXXX with the actual response_key>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6TQlhbl8qK7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TQlhbl8qK7f",
    "outputId": "f6d00d61-0f37-4c1e-e09b-063d57cc6e5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : This function will create an encrypted format of your given endpoint and that will need to pass for vulnerability\\n                Analysis to AIShield . \\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description : This function will create an encrypted format of your given endpoint and that will need to pass for vulnerability\n",
    "                Analysis to AIShield . \n",
    "\n",
    "\"\"\"\n",
    "def generate_encrypted_enpoint(payload):\n",
    "\n",
    "    \"\"\"\n",
    "        Description: this fucntion will generate a encrypted endpoint string with respect to the given org id and\n",
    "                     given info required to access the endpoint ,for ease access for the vulnerability analysis\n",
    "\n",
    "        Input_parameter :\n",
    "                  payload : dict : the necessary parameters to acess the endpoint as an dictionary format\n",
    "\n",
    "        return_parameter :\n",
    "                  encrypted_endpoint : str\n",
    "    \"\"\"\n",
    "\n",
    "    aishield_encrypt_url = \"https://api.aws.boschaishield.com/enc/api/ais/AIShieldModelEndpointEncryption/EncryptionService\"\n",
    "    encrypted_endpoint = \"\"\n",
    "    try:\n",
    "        enc_request = requests.request(method=\"POST\", url=aishield_encrypt_url, json=payload)\n",
    "        enc_request = json.loads(enc_request.text)\n",
    "        encrypted_endpoint = enc_request['model_api_details_encrypted_string']\n",
    "        print(\"model_api_details_encrypted_string : \",  encrypted_endpoint)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to encrypt the endpoint , please check the payload or contact AIShield \", enc_request, str(e))\n",
    "\n",
    "    return encrypted_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4612123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : Call the generate_encrypted_enpoint fucntion to generate the encrypted endpoint string for further use \n",
    "              in vulnerability analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if use_model_api.lower() == \"yes\":\n",
    "    \n",
    "    payload = {\"org_id\" : org_id,\n",
    "                \"url\" : model_hosted_endpoint_url,\n",
    "                \"accept_datatype\": accept_datatype,\n",
    "                \"input_key\" : input_key,\n",
    "                \"response_key\": response_key,\n",
    "                \"cert_pem_contents\": cert_pem_contents}\n",
    "\n",
    "\n",
    "    # Call the generate_encrypted_enpoint fucntion to generate the encrypted endpoint string\n",
    "    encrypted_endpoint = generate_encrypted_enpoint(payload=payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251ad457",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:28:37.208991Z",
     "iopub.status.busy": "2023-06-05T18:28:37.208991Z",
     "iopub.status.idle": "2023-06-05T18:28:38.353099Z",
     "shell.execute_reply": "2023-06-05T18:28:38.349996Z"
    },
    "hide_code": false,
    "id": "251ad457",
    "outputId": "11c31d5c-2015-486f-d31b-ba064ff98716",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: Specify the appropriate configs required for vulnerability analysis and trigger model analysis\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Specify the appropriate configs required for vulnerability analysis and trigger model analysis\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Description: Payload for AIShield VulnerabilityReport api call\n",
    "\"\"\"\n",
    "payload={}\n",
    "payload['use_model_api']=use_model_api\n",
    "\n",
    "if use_model_api.lower() == \"yes\":\n",
    "    payload['model_api_details']=encrypted_endpoint\n",
    "\n",
    "else:\n",
    "    payload['model_api_details']=\"no\"\n",
    "\n",
    "payload['normalize_data']=\"Yes\"\n",
    "payload['input_dimensions']=str(input_shape)\n",
    "payload['number_of_classes']=str(num_classes)\n",
    "payload['attack_type']=\"blackbox\"\n",
    "payload['number_of_attack_queries']=6000\n",
    "payload['model_framework']='tensorflow'\n",
    "payload['vulnerability_threshold']=\"0\"\n",
    "payload['defense_bestonly']=\"no\"\n",
    "payload['encryption_strategy']= model_encryption\n",
    "\n",
    "\"\"\"\n",
    "Description: Hit AIShield VulnerabilityReport api\n",
    "\"\"\"\n",
    "model_analysis_url = url + \"/model_analyse/{}\".format(model_id)\n",
    "if data_upload_status == \"upload sucessful\" and label_upload_status == \"upload sucessful\":\n",
    "    new_request = requests.request(method=\"POST\", url=model_analysis_url, json=payload,headers=headers)\n",
    "    new_request=json.loads(new_request.text)\n",
    "    for k, v in new_request.items():\n",
    "        print(\"* {} : {}\".format(k,v))\n",
    "\n",
    "#     print(new_request)\n",
    "    job_id=new_request['job_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6e269",
   "metadata": {
    "id": "78c6e269",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c938b",
   "metadata": {
    "id": "f46c938b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.5 Monitor Analysis Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55003c2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:28:38.360112Z",
     "iopub.status.busy": "2023-06-05T18:28:38.360112Z",
     "iopub.status.idle": "2023-06-05T18:33:26.590260Z",
     "shell.execute_reply": "2023-06-05T18:33:26.588183Z"
    },
    "hide_code": false,
    "id": "55003c2e",
    "outputId": "f9168c21-ba38-4206-a369-4d21880b4f62",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelExploration_Status : inprogress\n",
      "ModelExploration_Status : completed\n",
      "SanityCheck_Status : passed\n",
      "QueryGenerator_Status : inprogress\n",
      "QueryGenerator_Status : completed\n",
      "VunerabilityEngine_Status : inprogress\n",
      "VunerabilityEngine_Status : completed\n",
      "DefenseReport_Status : inprogress\n",
      "DefenseReport_Status : completed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Fetch Job status using Job ID\n",
    "\"\"\"\n",
    "\n",
    "def monitor_api_progress(new_job_id):\n",
    "    job_status_url = url + \"/job_status_detailed?job_id=\" + new_job_id\n",
    "\n",
    "    # status dictionary\n",
    "    status_dictionary = {\n",
    "        'ModelExploration_Status': 'na',\n",
    "        'SanityCheck_Status': 'na',\n",
    "        'QueryGenerator_Status': 'na',\n",
    "        'VunerabilityEngine_Status': 'na',\n",
    "        'DefenseReport_Status': 'na',\n",
    "    }\n",
    "    counts = [0] * len(status_dictionary)\n",
    "    failed_api_hit_count = 0\n",
    "    while True:\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            job_status_response = requests.request(\"GET\", job_status_url, params={},\n",
    "                                                   headers=headers)\n",
    "\n",
    "            job_status_payload = json.loads(job_status_response.text)\n",
    "            failing_key = 'ModelExploration_Status'\n",
    "            for i, key in enumerate(status_dictionary.keys()):\n",
    "                if status_dictionary[key] == 'na':\n",
    "                    if job_status_payload[key] == 'inprogress' and status_dictionary[key] == 'na':\n",
    "                        status_dictionary[key] = job_status_payload[key]\n",
    "                        print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                    elif job_status_payload[key] == 'completed' or job_status_payload[key] == 'passed':\n",
    "                        status_dictionary[key] = job_status_payload[key]\n",
    "                        counts[i] += 1\n",
    "                        print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                    if job_status_payload[key] == 'failed':\n",
    "                        failing_key = key\n",
    "                        status_dictionary[key] = job_status_payload[key]\n",
    "                        print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                elif job_status_payload[key] == 'completed' or job_status_payload[key] == 'passed':\n",
    "                    status_dictionary[key] = job_status_payload[key]\n",
    "                    if counts[i] < 1:\n",
    "                        print(str(key), \":\", status_dictionary[key])\n",
    "                    counts[i] += 1\n",
    "\n",
    "                else:\n",
    "                    if job_status_payload[key] == 'failed':\n",
    "                        failing_key = key\n",
    "                        status_dictionary[key] = job_status_payload[key]\n",
    "                        print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "            if job_status_payload[failing_key] == 'failed':\n",
    "                break\n",
    "\n",
    "            if status_dictionary['VunerabilityEngine_Status'] == 'passed' or status_dictionary[\n",
    "                'VunerabilityEngine_Status'] == 'completed' and job_status_payload[\n",
    "                'CurrentStatus'] == \"Defense generation is not triggered\":\n",
    "                print(\"\\n Vulnerability score {} failed to cross vulnerability threshold of {}\".format(\n",
    "                    job_status_payload['VulnerabiltyScore'], payload['vulnerability_threshold']))\n",
    "                break\n",
    "            if job_status_payload['DefenseReport_Status'] == 'completed':\n",
    "                break\n",
    "        except Exception as e:\n",
    "            failed_api_hit_count += 1\n",
    "            print(\"Error {}. trying {} ...\".format(str(e), failed_api_hit_count))\n",
    "            if failed_api_hit_count >= 3:\n",
    "                break\n",
    "    return status_dictionary\n",
    "\n",
    "status_dictionary = monitor_api_progress(new_job_id=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066faf3e",
   "metadata": {
    "id": "066faf3e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27f7b1",
   "metadata": {
    "id": "0f27f7b1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.6 Download Reports and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56bd0d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:26.599519Z",
     "iopub.status.busy": "2023-06-05T18:33:26.598522Z",
     "iopub.status.idle": "2023-06-05T18:33:26.617593Z",
     "shell.execute_reply": "2023-06-05T18:33:26.615601Z"
    },
    "id": "56bd0d06",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "report_path = \"reports/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f8a5a01",
   "metadata": {
    "id": "1f8a5a01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_artifact(job_id, report_path, report_type='Vulnerability', file_format=0):\n",
    "    \"\"\"\n",
    "    job_id: job_id  received after successful api call\n",
    "    report_type: report to be downloaded\n",
    "    file_format: change file_format to : 0- all report in zip\n",
    "                        1- report in .txt\n",
    "                        2- report in .pdf\n",
    "                        3- report in .json\n",
    "                        4- report in .xml\n",
    "    \"\"\"\n",
    "    print(\"received report_type : {} and file format is: {}\".format(report_type, file_format))\n",
    "    report_url = url + \"/\" + \"get_report?job_id=\" + str(\n",
    "        job_id) + \"&report_type=\" + report_type + \"&file_format=\" + str(file_format)\n",
    "\n",
    "    headers1 = headers\n",
    "    headers1[\"content-type\"] = \"application/zip\"\n",
    "\n",
    "    response = requests.request(\"GET\", report_url, params={}, headers=headers1)\n",
    "\n",
    "    file_path = None\n",
    "    if file_format == 0 or file_format == \"Attack_samples\":\n",
    "        file_path=os.path.join(report_path, report_type + \".zip\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 1:\n",
    "        file_path=os.path.join(report_path, report_type + \".txt\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 2:\n",
    "        file_path=os.path.join(report_path, report_type + \".pdf\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 3:\n",
    "        file_path=os.path.join(report_path, report_type + \".json\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    elif file_format == 4:\n",
    "        file_path=os.path.join(report_path, report_type + \".xml\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce56da2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:26.623599Z",
     "iopub.status.busy": "2023-06-05T18:33:26.622603Z",
     "iopub.status.idle": "2023-06-05T18:33:29.924086Z",
     "shell.execute_reply": "2023-06-05T18:33:29.922601Z"
    },
    "hide_code": false,
    "id": "ce56da2b",
    "outputId": "f94d489e-dfcb-404d-a69f-be6f6bde2bbf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received report_type : Vulnerability and file format is: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Defense Reports\n",
    "\"\"\"\n",
    "vul_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Vulnerability', file_format=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "094f5a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:29.927533Z",
     "iopub.status.busy": "2023-06-05T18:33:29.927533Z",
     "iopub.status.idle": "2023-06-05T18:33:30.753174Z",
     "shell.execute_reply": "2023-06-05T18:33:30.751183Z"
    },
    "hide_code": false,
    "id": "094f5a93",
    "outputId": "3b6e8951-aad7-4477-930d-91af74add3bf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received report_type : Defense and file format is: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Defense Reports\n",
    "\"\"\"\n",
    "def_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Defense', file_format=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dda4e59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:30.755901Z",
     "iopub.status.busy": "2023-06-05T18:33:30.755901Z",
     "iopub.status.idle": "2023-06-05T18:33:31.600939Z",
     "shell.execute_reply": "2023-06-05T18:33:31.598946Z"
    },
    "hide_code": false,
    "id": "6dda4e59",
    "outputId": "99dd1e51-0f43-437c-c91c-d2e966f5007f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received report_type : Defense_artifact and file format is: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Defense artifacts: Model\n",
    "\"\"\"\n",
    "def_artifact_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Defense_artifact', file_format=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77c57373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:31.607651Z",
     "iopub.status.busy": "2023-06-05T18:33:31.607651Z",
     "iopub.status.idle": "2023-06-05T18:33:32.603112Z",
     "shell.execute_reply": "2023-06-05T18:33:32.600361Z"
    },
    "hide_code": false,
    "id": "77c57373",
    "outputId": "4710655b-341d-4689-a007-40e22d5d1112",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received report_type : Attack_samples and file format is: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Download the Attack Samples\n",
    "\"\"\"\n",
    "\n",
    "attack_report = download_artifact(job_id=job_id, report_path= report_path, report_type='Attack_samples', file_format=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "Odv5kzeYFTOZ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:32.606419Z",
     "iopub.status.busy": "2023-06-05T18:33:32.606419Z",
     "iopub.status.idle": "2023-06-05T18:33:32.645220Z",
     "shell.execute_reply": "2023-06-05T18:33:32.644077Z"
    },
    "hide_code": false,
    "id": "Odv5kzeYFTOZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_file_id(report_path):\n",
    "\n",
    "    file_id = None\n",
    "    !pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "\n",
    "    import os\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "    # Authenticate and authorize access to the Google Drive API\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "    # Build the Drive API service\n",
    "    drive_service = build('drive', 'v3')\n",
    "\n",
    "    # Upload the PDF file to Google Drive\n",
    "    pdf_file_name = os.path.basename(report_path)\n",
    "    file_metadata = {'name': pdf_file_name}\n",
    "    media = MediaFileUpload(report_path, mimetype='application/pdf')\n",
    "    file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    # Set sharing permissions to \"Anyone with the link\"\n",
    "    file_id = file.get('id')\n",
    "    drive_service.permissions().create(\n",
    "        fileId=file_id,\n",
    "        body={'role': 'reader', 'type': 'anyone'}\n",
    "    ).execute()\n",
    "\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f2c29ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-06-05T18:33:32.652937Z",
     "iopub.status.busy": "2023-06-05T18:33:32.652937Z",
     "iopub.status.idle": "2023-06-05T18:33:32.679968Z",
     "shell.execute_reply": "2023-06-05T18:33:32.678696Z"
    },
    "hide_code": false,
    "id": "6f2c29ad",
    "outputId": "22115b3f-3fa6-44ee-c0af-e647e9587ccf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.108.0-py2.py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.19.6)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Installing collected packages: google-auth-oauthlib, google-api-python-client\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.6\n",
      "    Uninstalling google-auth-oauthlib-0.4.6:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.84.0\n",
      "    Uninstalling google-api-python-client-2.84.0:\n",
      "      Successfully uninstalled google-api-python-client-2.84.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.9.1 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 1.1.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.9.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
      "tensorflow 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.9.1 which is incompatible.\n",
      "tensorflow 2.14.0 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-python-client-2.108.0 google-auth-oauthlib-1.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "googleapiclient"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/1FRxTzustCHwqcEXmLpOJjGzwCtuprhvG/preview\" width=\"100%\" height=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Displaying the current Vulnerability Assement report\n",
    "\"\"\"\n",
    "\n",
    "if (\"colab\" in runtime_environment.lower()):\n",
    "    file_id = get_file_id(report_path = vul_report)\n",
    "    display_pdf(output_filename = \"vul_\"+job_id[-10:],runtime_environment=runtime_environment,file_id=file_id)\n",
    "else:\n",
    "    display_pdf(output_filename = \"vul_\"+job_id[-10:],runtime_environment=runtime_environment, pdf_path =os.path.join(report_path, os.path.basename(vul_report)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f67ff9a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hide_code": true,
    "id": "f67ff9a4",
    "outputId": "6639a093-8660-427c-84c5-3523ee91096f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.108.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.19.6)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/1CtgYLvz9TADXo_ZNLJoF2j83bxhW-1AN/preview\" width=\"100%\" height=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Displaying the current Defense Assement report\n",
    "\"\"\"\n",
    "\n",
    "if (\"colab\" in runtime_environment.lower()):\n",
    "    file_id = get_file_id(report_path = def_report)\n",
    "    display_pdf(output_filename = \"Defense_\"+job_id[-10:],runtime_environment=runtime_environment,file_id=file_id)\n",
    "else:\n",
    "    display_pdf(output_filename = \"Defense_\"+job_id[-10:],runtime_environment=runtime_environment, pdf_path =os.path.join(report_path, os.path.basename(def_report)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6693e",
   "metadata": {
    "id": "d3b6693e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://aisdocs.blob.core.windows.net/reference/Workflow Images/aw6.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450fbca",
   "metadata": {
    "id": "c450fbca",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### After conducting a Vulnerability Analysis by AIshield, if a defense recommendation is identified, then following actions can be taken:\n",
    "\n",
    "1. **Model Retraining:** It is advised to retrain the original model to enhance its robustness. By revisiting the training process and incorporating the insights gained from the vulnerability analysis, the model can be improved to better handle potential threats and mitigate vulnerabilities.\n",
    "\n",
    "\n",
    "2. **Deploying Threat-Informed Defense Model:** In addition to the original model, AIshield provides a specialized defense model that is specifically designed to address the identified threats. This threat-informed defense model can be deployed alongside the original model to provide an additional layer of protection and enhance the overall security posture. By leveraging the insights gained from the vulnerability analysis, the defense model offers targeted defenses against potential threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b21f8",
   "metadata": {
    "id": "588b21f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "codeState": [
   "",
   "",
   "",
   "none",
   "none",
   "none",
   "none",
   "none",
   "none",
   "none",
   "",
   "none",
   "none",
   "",
   "",
   "",
   "",
   "",
   "none",
   "",
   "",
   "none",
   "",
   "",
   "",
   "",
   "",
   "",
   "",
   "",
   "",
   "",
   "",
   "",
   "none",
   "",
   ""
  ],
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "custom_metadata": {
   "show_hide_code": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
